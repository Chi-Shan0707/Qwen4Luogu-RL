import torch
import os
import re
import json
import subprocess
import tempfile
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ==================== é…ç½®åŒºåŸŸ ====================
MS_MODEL_ID = "qwen/Qwen2.5-Coder-3B-Instruct"
LOCAL_MODEL_DIR = "./models/Qwen2.5-Coder-3B-Instruct"

# ã€å…³é”®ã€‘ç›´æ¥ä½¿ç”¨ä½ æä¾›çš„ JSON æ•°æ®ç»“æ„è¿›è¡Œæµ‹è¯•
# è¿™é‡Œä½¿ç”¨äº† P1029 [NOIP 2001 æ™®åŠç»„] ä½œä¸ºæµ‹è¯•é¢˜
TEST_DATA_JSON = {
    "prompt": "ä½ å°†å¾—åˆ°ä¸€ä¸ªç¼–ç¨‹ç«èµ›é¢˜ç›®ã€‚è¯·é€æ­¥æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œç„¶åç”¨Cæˆ–C++æä¾›å®Œæ•´çš„å®ç°ã€‚è¯·å‹¿åŒ…å«ä»»ä½•è°ƒè¯•ä¿¡æ¯æˆ–é¢å¤–è¾“å‡ºã€‚å°†æœ€ç»ˆè§£å†³æ–¹æ¡ˆæ”¾åœ¨å•ä¸ªä»£ç å—ä¸­ï¼š\n```cpp\n<your code here>\n```\n\n\né¢˜ç›®å†…å®¹:\n\n# P1029 [NOIP 2001 æ™®åŠç»„] æœ€å¤§å…¬çº¦æ•°å’Œæœ€å°å…¬å€æ•°é—®é¢˜\n\n\n## é¢˜ç›®æè¿°\n\nè¾“å…¥ä¸¤ä¸ªæ­£æ•´æ•° $x_0, y_0$ï¼Œæ±‚å‡ºæ»¡è¶³ä¸‹åˆ—æ¡ä»¶çš„ $P, Q$ çš„ä¸ªæ•°ï¼š\n1. $P,Q$ æ˜¯æ­£æ•´æ•°ã€‚\n2. è¦æ±‚ $P, Q$ ä»¥ $x_0$ ä¸ºæœ€å¤§å…¬çº¦æ•°ï¼Œä»¥ $y_0$ ä¸ºæœ€å°å…¬å€æ•°ã€‚\nè¯•æ±‚ï¼šæ»¡è¶³æ¡ä»¶çš„æ‰€æœ‰å¯èƒ½çš„ $P, Q$ çš„ä¸ªæ•°ã€‚\n\n## è¾“å…¥æ ¼å¼\n\nä¸€è¡Œä¸¤ä¸ªæ­£æ•´æ•° $x_0, y_0$ã€‚\n\n## è¾“å‡ºæ ¼å¼\n\nä¸€è¡Œä¸€ä¸ªæ•°ï¼Œè¡¨ç¤ºæ±‚å‡ºæ»¡è¶³æ¡ä»¶çš„ $P, Q$ çš„ä¸ªæ•°ã€‚\n\n## è¯´æ˜/æç¤º\n\n$P,Q$ æœ‰ $4$ ç§ï¼š\n1. $3, 60$ã€‚\n2. $15, 12$ã€‚\n3. $12, 15$ã€‚\n4. $60, 3$ã€‚\nå¯¹äº $100\\%$ çš„æ•°æ®ï¼Œ$2 \\le x_0, y_0 \\le {10}^5$ã€‚\n**ã€é¢˜ç›®æ¥æºã€‘**\nNOIP 2001 æ™®åŠç»„ç¬¬äºŒé¢˜\n\n## æ ·ä¾‹\n\n### æ ·ä¾‹ 1\n\n**è¾“å…¥ï¼š**\n```\n3 60\n```\n\n**è¾“å‡ºï¼š**\n```\n4\n```\n\n",
    "test_cases": [{"input": "3 60", "output": "4"}]
}
# =================================================

def print_step(title):
    print(f"\n{'='*10} {title} {'='*10}")

def extract_code(completion):
    """ä»å›å¤ä¸­æå–ä»£ç ï¼Œé€»è¾‘åŒ train_rl.py"""
    # ä¼˜å…ˆåŒ¹é…ä»£ç å—
    match = re.search(r"```(?:cpp|c\+\+)?\n(.*?)```", completion, re.DOTALL)
    if match:
        return match.group(1), "Code Block"
    # å…œåº•åŒ¹é… #include
    elif "#include" in completion:
        return completion, "Raw Text"
    else:
        return None, "Failed"

def compile_and_run(code, test_cases):
    """ç¼–è¯‘å¹¶è¿è¡Œï¼Œé€»è¾‘åŒ train_rl.py"""
    # ç§»é™¤ freopenï¼Œé˜²æ­¢å¡æ­»
    code = re.sub(r'freopen\s*\(.*?\);', '', code, flags=re.IGNORECASE)
    
    with tempfile.TemporaryDirectory() as temp_dir:
        src_file = os.path.join(temp_dir, "solution.cpp")
        exe_file = os.path.join(temp_dir, "solution")
        
        # å†™å…¥
        with open(src_file, 'w', encoding='utf-8') as f:
            f.write(code)
            
        print(f"   -> æ­£åœ¨ç¼–è¯‘ä¸´æ—¶æ–‡ä»¶...")
        # ç¼–è¯‘
        try:
            res = subprocess.run(
                ['g++', src_file, '-o', exe_file, '-O2'],
                capture_output=True, text=True, timeout=5
            )
            if res.returncode != 0:
                return 0.0, f"ç¼–è¯‘å¤±è´¥:\n{res.stderr}"
        except Exception as e:
            return 0.0, f"ç¼–è¯‘å¼‚å¸¸: {e}"

        # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        passed = 0
        total = len(test_cases)
        for i, case in enumerate(test_cases):
            input_data = case['input']
            expected_output = case['output'].strip()
            
            try:
                res = subprocess.run(
                    [exe_file],
                    input=input_data,
                    capture_output=True,
                    text=True,
                    timeout=2 # 2ç§’è¶…æ—¶
                )
                actual_output = res.stdout.strip()
                
                if actual_output == expected_output:
                    print(f"   -> Case {i+1}: âœ… é€šè¿‡ (è¾“å…¥: '{input_data.strip()}' | é¢„æœŸ: '{expected_output}' | å®é™…: '{actual_output}')")
                    passed += 1
                else:
                    print(f"   -> Case {i+1}: âŒ å¤±è´¥ (è¾“å…¥: '{input_data.strip()}' | é¢„æœŸ: '{expected_output}' | å®é™…: '{actual_output}')")
            except subprocess.TimeoutExpired:
                print(f"   -> Case {i+1}: âš ï¸ è¿è¡Œè¶…æ—¶ (Timeout)")
            except Exception as e:
                print(f"   -> Case {i+1}: âš ï¸ è¿è¡Œé”™è¯¯ {e}")
        
        return passed / total, "Success"

def main():
    print_step("STEP 1: åŠ è½½æ¨¡å‹ä¸Tokenizer")
    
    # æ£€æŸ¥ g++
    try:
        subprocess.run(['g++', '--version'], capture_output=True)
        print("âœ… æ£€æµ‹åˆ° g++ ç¼–è¯‘å™¨")
    except:
        print("âŒ æœªæ£€æµ‹åˆ° g++ï¼Œè¯·å…ˆå®‰è£… (sudo apt install g++)")
        return

    # åŠ è½½ Tokenizer
    model_path = LOCAL_MODEL_DIR if os.path.exists(LOCAL_MODEL_DIR) else MS_MODEL_ID
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ (4-bit)
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path} (4-bit)...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # ------------------------------------------------------------------
    print_step("STEP 2: éªŒè¯ Chat Template (JSON -> Qwen Prompt)")
    
    # æ¨¡æ‹Ÿ train_rl.py ä¸­çš„æ•°æ®å¤„ç†é€»è¾‘
    raw_prompt = TEST_DATA_JSON['prompt']
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ã€‚æ¨ç†éƒ¨åˆ†å†…å®¹æ§åˆ¶åœ¨128tokenä»¥å†…ã€‚ä»£ç è¦ä¸¥æ ¼æŒ‰ç…§ä¼ ç»Ÿc++ç¼–å†™ã€‚"},
        {"role": "user", "content": raw_prompt}
    ]
    
    # åº”ç”¨æ¨¡ç‰ˆ
    final_prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    print("--- æœ€ç»ˆè¾“å…¥ç»™æ¨¡å‹çš„ Prompt å¼€å¤´éƒ¨åˆ† ---")
    print(final_prompt[:300] + "...\n")
    print("--- æœ€ç»ˆè¾“å…¥ç»™æ¨¡å‹çš„ Prompt ç»“å°¾éƒ¨åˆ† ---")
    print("..." + final_prompt[-100:])
    
    # æ£€æŸ¥å…³é”®æ ‡ç­¾
    if "<|im_start|>system" in final_prompt and "<|im_start|>assistant" in final_prompt:
        print("\nâœ… æ¨¡ç‰ˆæ ¼å¼æ£€æŸ¥é€šè¿‡ (æ£€æµ‹åˆ° Qwen ChatML æ ‡ç­¾)")
    else:
        print("\nâŒ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° ChatML æ ‡ç­¾ï¼Œè¯·æ£€æŸ¥ tokenizer_config.json")

    # ------------------------------------------------------------------
    print_step("STEP 3: æ‰§è¡Œæ¨¡å‹ç”Ÿæˆ")
    
    inputs = tokenizer([final_prompt], return_tensors="pt").to(model.device)
    
    print(f"Prompt token é•¿åº¦: {inputs.input_ids.shape[1]}")
    print("æ­£åœ¨ç”Ÿæˆ (Max 1024 tokens)...")
    
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=True,     
            temperature=0.6,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id
        )
    
    # è§£ç 
    full_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]
    
    # åªè¦ç”Ÿæˆéƒ¨åˆ†
    if "<|im_start|>assistant" in full_response:
        response_only = full_response.split("<|im_start|>assistant")[-1]
    else:
        response_only = full_response
    
    print("\n--- æ¨¡å‹ç”Ÿæˆçš„ä»£ç éƒ¨åˆ† (å‰1000å­—ç¬¦) ---")
    print(response_only[:1000] + "..." if len(response_only)>500 else response_only)

    # ------------------------------------------------------------------
    print_step("STEP 4: éªŒè¯ä»£ç æå–ä¸è¯„æµ‹ (åŸºäº JSON test_cases)")
    
    extracted_code, method = extract_code(response_only)
    test_cases = TEST_DATA_JSON['test_cases']
    
    if extracted_code:
        print(f"âœ… æˆåŠŸæå–ä»£ç  (æ–¹å¼: {method})")
        print(f"æ­£åœ¨ä½¿ç”¨ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„æµ‹...")
        
        # å®é™…è¿è¡Œè¯„æµ‹
        score, msg = compile_and_run(extracted_code, test_cases)
        
        print(f"\nğŸ“Š æœ€ç»ˆå¾—åˆ† (Reward): {score}")
        
        if score == 1.0:
            print("ğŸ‰ ç»“è®ºï¼šPipeline å®Œç¾é€šè¿‡ï¼æ¨¡å‹æˆåŠŸè§£å‡ºäº†é¢˜ç›®ã€‚")
        elif score > 0.0:
            print("âš ï¸ ç»“è®ºï¼šPipeline é€šç•…ï¼Œä»£ç å¯è¿è¡Œï¼Œä½†éƒ¨åˆ†ç”¨ä¾‹æœªé€šè¿‡ (è¿™æ˜¯ RL è®­ç»ƒéœ€è¦è§£å†³çš„é—®é¢˜)ã€‚")
        else:
            print(f"âš ï¸ ç»“è®ºï¼šä»£ç ç¼–è¯‘å¤±è´¥æˆ–è¿è¡Œå…¨é”™ã€‚è¯¦ç»†ä¿¡æ¯: {msg}")
            print("æ³¨æ„ï¼šå¯¹äºæœªå¾®è°ƒçš„ 3B æ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡åšå¯¹ä¸­ç­‰éš¾åº¦çš„æ•°è®ºé¢˜(P1029)æ˜¯æœ‰æŒ‘æˆ˜çš„ã€‚åªè¦ç¼–è¯‘è¿‡ç¨‹æ²¡æŠ¥é”™ï¼ŒPipeline å°±æ˜¯å¥½çš„ã€‚")
    else:
        print("âŒ ä»£ç æå–å¤±è´¥ï¼æ¨¡å‹å¯èƒ½æ²¡ç”Ÿæˆä»£ç å—ã€‚")

if __name__ == "__main__":
    main()