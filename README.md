# LuoguQwen-RL â€” TinyLoRA Experiment

<div align="center">

**å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¶…å‚æ•°å‹ç¼©æ¨¡å‹ï¼šQwen2.5-Coder on Luogu OJ**

[ä¸­æ–‡ç‰ˆæœ¬](#ä¸­æ–‡ç‰ˆæœ¬) | [English Version](#english-version)

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæˆ–è€…ä½ è§‰å¾—æœ‰ç‚¹æ„æ€ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ Star æ”¯æŒä¸€ä¸‹ï¼è¿™å¯¹æˆ‘å¾ˆé‡è¦ï¼Œä¸‡åˆ†æ„Ÿè°¢PwPï¼<br>
If you find this project useful or interesting, please give it a Star! ğŸŒŸ Your support is my greatest motivation.<br>
</div>

---

# ä¸­æ–‡ç‰ˆæœ¬

## LuoguQwen-RL â€” TinyLoRA å®éªŒ

æœ¬ä»“åº“æ˜¯åŸã€ŒLuoguQwen LoRA å¾®è°ƒã€ï¼Œä¸€ä¸ª[åŸºäº SFTçš„é¡¹ç›®](https://github.com/Chi-Shan0707/Qwen4Luogu-SFT)çš„è¿›åŒ–ç‰ˆï¼š

> ä»€ä¹ˆï¼Œä½ é—®æˆ‘ä¸ºä»€ä¹ˆè¦æŒ‘é€‰ Qwen2.5-1.5B-Instruct è¿›è¡Œå¾®è°ƒï¼Ÿ<br>
> â€”â€” é‚£å½“ç„¶æ˜¯å› ä¸ºå®ƒå‚æ•°é‡å°å•¦ã€‚<br>
>
> ä»€ä¹ˆï¼Œä½ ç»§ç»­é—®æˆ‘ä¸ºä»€ä¹ˆä¸æŒ‘é€‰ Qwen2.5-Coder-1.5B-Instruct è¿›è¡Œå¾®è°ƒï¼Ÿ<br>
> ~~æˆ‘å¦‚æœåœ¨è¿™é˜¿é‡Œè¿›è¡Œè¿‡ä»£ç è®­ç»ƒä¸Šçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå“ªèƒ½çœ‹å¾—å‡ºæˆ‘å¾®è°ƒçš„æ•ˆæœï¼Ÿ~~<br>
> ~~å¥½å§ï¼Œå…¶å®æ˜¯æˆ‘é—®åƒé—®æœ‰ä»€ä¹ˆå‚æ•°é‡å°çš„æ¨¡å‹ï¼Œå®ƒæ¨èäº†è¿™ä¸ªï¼Œç„¶åæˆ‘ä¸€æ—¶é—´å¿˜è®°ç»§ç»­å»æœé›†ä¿¡æ¯ï¼Œç›´æ¥å¼€ææƒ¹ï¼Œç»“æœè®­ç»ƒåˆ°ä¸€åŠæ‰åœ¨ ModelScope ä¸Šåˆ·åˆ° Qwen2.5-Coder-1.5B-Instructã€‚PWP~~<br>
> ~~ç¬¬ä¸€éå®åœ¨å¤ªå·®äº†ï¼Œåæ­£è¿˜è¦å†è®­ç»ƒä¸€éï¼Œè¿˜æ˜¯å¼„ Qwen2.5-Coder-1.5B-Instruct å§~~<br>
> è¿™ä¸ªä¹Ÿå¤ªå·®åŠ²äº†ï¼Œä¸Š 7B å§ PwP<br>
> *ä¸å¯¹ï¼Œä¸ºä»€ä¹ˆç–¯ç‹‚æŠ¥ mismatch å•Šå•Šï¼Ÿä» 1.5Bâ†’7B æˆ‘å•¥éƒ½æ²¡æ”¹å•Šï¼Ÿ*<br>
> *ç–¯ç‹‚ debugï¼Œç–¯ç‹‚ç ”ç©¶æ ¼å¼â€¦â€¦*<br>
> ç®—äº†ï¼Œæ ¼å¼å¼„æˆæ‰€è°“çš„æ ‡å‡†å‹å§ã€‚<br>
> 7B æ ¹æœ¬è·‘ä¸åŠ¨å•Šï¼Œåªèƒ½ 3Bã€‚<br>
> ~~å•Šè®­ç»ƒå®Œäº†ï¼Œå‚æ•°æ ¹æœ¬ä¸Šä¼ ä¸åŠ¨å•Šï¼Ÿå•Šï¼Œhuggingface ä¹Ÿä¸Šä¼ ä¸åŠ¨å•Š PwP~~<br>

ç„¶åï¼Œ6å·æ™šä¸Šï¼Œ~~å¤©åŠ©æˆ‘ä¹Ÿ~~ï¼Œæˆ‘çœ‹åˆ°äº†TinyLoRAçš„è®ºæ–‡ï¼Œæ‰€ä»¥æˆ‘å°±å¼€å§‹äº†è¿™é¡¹å°è¯•ï¼ˆæˆ–è€…å¯ä»¥è¯´â€œå¤ç°â€ï¼‰ï¼š
- åŸºåº§ï¼šQwen2.5-Coder-3B-Instructï¼Œ4bit é‡åŒ–ä»¥æŒ¤çˆ†æœ€åä¸€ç‚¹æ˜¾å­˜ï¼›
- è®­ç»ƒï¼šä¸ç”¨ SFTï¼Œç”¨ RLï¼ˆGRPOï¼‰ï¼›
- å‚æ•°ï¼šå…¨æ¨¡å‹åªä¿ç•™ **16 ä¸ªå¯è®­ç»ƒæ ‡é‡å‚æ•°**ï¼›
- ä»»åŠ¡ï¼šç”¨ã€Œç¼–è¯‘+è¿è¡Œ C++ ä»£ç ã€çš„æ–¹å¼åœ¨æ´›è°·é¢˜ç›®ä¸Šæä»£ç å¼ºåŒ–å­¦ä¹ ã€‚
<br>

ç›®å‰ï¼Œè¿™ä¸ª`train_rl.py`æ˜¯å¯ä»¥è¿è¡Œä¸”è®­ç»ƒçš„ï¼Œä½†æ˜¯èƒ½æˆåŠŸè¿è¡Œ+é€šè¿‡æ ·ä¾‹æµ‹è¯•çš„ï¼Œåä¸å­˜ä¸€ï¼ˆå¹¶æ²¡æœ‰å¤¸å¼ ï¼‰ã€‚<br>
åŸå› å¯èƒ½æœ‰:
- æç¤ºè¯å†™çš„ä¸å¥½ï¼Œä¸‹ä¸€æ­¥éœ€è¦æ˜ç¡®ã€æ˜¯å¦è¦æ¨ç†è·¯å¾„ã€‘ç­‰ç»†èŠ‚ï¼Œå¹¶å¼€å±•Prompt Engineering
- tokenæ•°é‡æˆªå–çš„å¤ªå°‘ï¼Œç›®å‰æ˜¯1024ï¼Œä½†æ˜¯è¿™ä¸ªä¹Ÿä¼šå¸¦æ¥æˆæœ¬
- GRPOæ—¶ç”Ÿæˆç­”æ¡ˆæ•°é‡å¤ªå°‘
- luogué¢˜ç›®å¤ªéš¾
- RLçš„rewardå†™çš„ä¸å¤Ÿå¥½
- 3Bæ¨¡å‹æ¯”è¾ƒå·®<br>

<br>

`train_rl.py`ä¸­æ”¯æŒä¿®æ”¹
- æ›´æ¢æ¨¡å‹ï¼Œå¦‚é‡‡ç”¨Qwen2.5-7B
- GRPOçš„config
- reward
- ...

---

## ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [è®ºæ–‡å¤ç°](#è®ºæ–‡å¤ç°)
- [æ ¸å¿ƒç‰¹ç‚¹](#æ ¸å¿ƒç‰¹ç‚¹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ•°æ®å‡†å¤‡ä¸æ ¼å¼](#æ•°æ®å‡†å¤‡ä¸æ ¼å¼)
- [è®­ç»ƒæµç¨‹ï¼ˆRL / GRPOï¼‰](#è®­ç»ƒæµç¨‹rl--grpo)
- [TinyLoRA Tiling æŠ€æœ¯ç»†èŠ‚](#tinylora-tiling-æŠ€æœ¯ç»†èŠ‚)
- [å¥–åŠ±å‡½æ•°ï¼šç¼–è¯‘è¿è¡Œ C++ ä»£ç ](#å¥–åŠ±å‡½æ•°ç¼–è¯‘è¿è¡Œ-c-ä»£ç )
- [èµ„æºæ¶ˆè€—ä¸æ³¨æ„äº‹é¡¹](#èµ„æºæ¶ˆè€—ä¸æ³¨æ„äº‹é¡¹)
- [å¼€æºä¸è®¸å¯è¯](#å¼€æºä¸è®¸å¯è¯)
- [å¼•ç”¨](#å¼•ç”¨)


---

## é¡¹ç›®æ¦‚è¿°

LuoguQwen-RL çš„ç›®æ ‡æ˜¯ï¼š

> åœ¨æ˜¾å­˜å—é™ï¼ˆ3B æ¨¡å‹ + 4bit é‡åŒ–ï¼‰ä¸”å‚æ•°æè‡´å‹ç¼©ï¼ˆä»… 16 ä¸ªå‚æ•°ï¼‰çš„å‰æä¸‹ï¼Œ
> é€šè¿‡å¼ºåŒ–å­¦ä¹ è®© Qwen2.5-Coder åœ¨æ´›è°·ç«èµ›é¢˜ä¸Šå­¦ä¼šã€Œèƒ½è¿‡æ ·ä¾‹ã€çš„ C++ ä»£ç ç”Ÿæˆã€‚

æœ¬ä»“åº“å¹¶ä¸æ˜¯å‡­ç©ºè®¾è®¡çš„ï¼Œè€Œæ˜¯ä¸€ä¸ª**TinyLoRA è®ºæ–‡æ–¹å‘çš„å¤ç°ä¸å˜ä½“å®éªŒ**ï¼š

- `theory/README.md` ä¸­ç»™å‡ºäº† TinyLoRA / GRPO çš„ç†è®ºä¸å·¥ç¨‹ç»†èŠ‚æ¢³ç†ï¼›
- æœ¬é¡¹ç›®åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°† TinyLoRA çš„æ€æƒ³ä»æ•°å­¦æ¨ç†ï¼ˆå¦‚ GSM8Kï¼‰è¿ç§»åˆ°**ä»£ç ç”Ÿæˆ + ç¼–è¯‘æ‰§è¡Œå¥–åŠ±**åœºæ™¯ï¼›
- è®ºæ–‡ä¸­ç»å…¸è®¾ç½®æ˜¯ 7B æ¨¡å‹ + 13 ä¸ªå‚æ•°ï¼Œæœ¬ä»“åº“ä½¿ç”¨ 3B Coder æ¨¡å‹ + 16 ä¸ªå‚æ•°ï¼Œä¿æŒã€Œæä½ç§© + å…¨å±€å…±äº«ã€è¿™ä¸€ç²¾ç¥å†…æ ¸ã€‚

æ ¸å¿ƒè„šæœ¬ï¼š

- `train_rl.py`ï¼š
  - åŠ è½½ 4bit é‡åŒ–çš„ `Qwen2.5-Coder-3B-Instruct`ï¼›
  - å°†æŒ‡å®š Linear å±‚æ›¿æ¢ä¸ºè‡ªå®šä¹‰ `TinyLoRALinear`ï¼Œå¹¶é€šè¿‡å…±äº«å‘é‡ `global_v` å®ç° TinyLoRA Tilingï¼›
  - ä½¿ç”¨ TRL çš„ `GRPOTrainer` è¿›è¡Œä»£ç å¼ºåŒ–å­¦ä¹ ï¼›
  - å¥–åŠ±æ¥è‡ªæœ¬åœ° `g++` ç¼–è¯‘ + æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œé€šè¿‡ç‡ã€‚
- `convert_dataset.py`ï¼š
  - ä»æœ¬åœ°æ´›è°·é¢˜ç›®æ•°æ®ï¼ˆMarkdown é£æ ¼ï¼‰ä¸­ç”¨æ­£åˆ™æŠ½å– `prompt`ï¼ˆé¢˜é¢ï¼‰ä¸ `test_cases`ï¼ˆè¾“å…¥è¾“å‡ºæ ·ä¾‹ï¼‰ï¼›
  - è¿‡æ»¤æ‰åŒ…å«ä¸­æ–‡çš„æ ·ä¾‹ï¼Œè½¬å­˜ä¸º JSONLï¼Œä¾› RL è®­ç»ƒä½¿ç”¨ã€‚
- `download_dataset.py`ï¼š
  - ä» Hugging Face ä¸‹è½½ DPO æ ¼å¼çš„æ´›è°·æ•°æ®é›†å¹¶ä¿å­˜åˆ° `./local_luogu_dpo`ï¼ˆä¾› `convert_dataset.py` ä½¿ç”¨ï¼‰ã€‚
- `verify_pipeline.py`ï¼š
  - ç”¨äºéªŒè¯æ¨¡å‹åŠ è½½ã€ç”Ÿæˆã€ä»£ç æå–ä¸ç¼–è¯‘è¿è¡Œçš„ç«¯åˆ°ç«¯æµæ°´çº¿ï¼ˆç¤ºä¾‹ï¼šåŠ è½½æ¨¡å‹å¹¶å°è¯•ç”¨ç»™å®šæ ·ä¾‹å¯¹ç”Ÿæˆä»£ç è¿›è¡Œç¼–è¯‘è¿è¡Œè¯„æµ‹ï¼‰ã€‚

ç›®å½•ç»“æ„ï¼ˆèŠ‚é€‰ï¼‰ï¼š

- `train_rl.py`ï¼šä¸»è®­ç»ƒè„šæœ¬ï¼ˆTinyLoRA + GRPOï¼‰ã€‚
- `download_dataset.py`ï¼šä» Hugging Face ä¸‹è½½ DPO æ ¼å¼æ•°æ®å¹¶ä¿å­˜åˆ° `./local_luogu_dpo`ã€‚
- `verify_pipeline.py`ï¼šéªŒè¯ model->generate->extract->compile æµç¨‹çš„è„šæœ¬ã€‚
- `convert_dataset.py`ï¼šå°†æœ¬åœ°æ´›è°· DPO æ•°æ®è½¬ä¸º RL JSONL æ ¼å¼ã€‚
- `local_luogu_dpo/`ï¼šä»åŸ DPO æ•°æ®é›†è½¬å­˜çš„æœ¬åœ°æ•°æ®ï¼ˆ`load_from_disk` äº§ç‰©ï¼‰ã€‚
- `local_luogu_rl/luogu_rl_data.jsonl`ï¼šRL è®­ç»ƒæ•°æ®ï¼ˆ`convert_dataset.py` è¾“å‡ºï¼‰ã€‚
- `models/Qwen2.5-Coder-3B-Instruct/`ï¼šåŸºåº§æ¨¡å‹ç›®å½•ï¼ˆå¯é€šè¿‡ ModelScope è‡ªåŠ¨ä¸‹è½½ï¼‰ã€‚
- `output/`ï¼šRL è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆåŒ…æ‹¬æœ€ç»ˆçš„ `tiny_lora_v.pt`ï¼Œå†…å« `global_v` å‘é‡åŠé‡å»ºæ‰€éœ€çš„å…ƒä¿¡æ¯ï¼‰ã€‚

---
## è®ºæ–‡å¤ç°

[cite_start]æœ¬é¡¹ç›®æ˜¯è®ºæ–‡ **"Learning to Reason in 13 Parameters" (Morris et al., 2026)** çš„éå®˜æ–¹å¤ç°ä¸å·¥ç¨‹é€‚é… [cite: 2]ã€‚

### 1. æ ¸å¿ƒç†è®ºï¼šTinyLoRA
åŸè®ºæ–‡æå‡ºäº†ä¸€ç§æç«¯çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³• **TinyLoRA**ï¼Œæ—¨åœ¨æ‰“ç ´ LoRA çš„ç§©ï¼ˆRankï¼‰é™åˆ¶ã€‚
- [cite_start]**ç—›ç‚¹**ï¼šä¼ ç»Ÿ LoRA å³ä½¿ Rank=1ï¼Œå…¶å‚æ•°é‡ä»ä¸æ¨¡å‹å®½åº¦ $d$ æˆæ­£æ¯”ï¼ˆ$O(d \times r)$ï¼‰ï¼Œå¯¹äº 7B æ¨¡å‹çº¦ä¸ºæ•°ç™¾ä¸‡å‚æ•° [cite: 17, 158]ã€‚
- [cite_start]**åˆ›æ–°**ï¼šTinyLoRA åˆ©ç”¨ SVD å†»ç»“åŸæƒé‡çš„ç‰¹å¾æ–¹å‘ ($U, V$)ï¼Œä»…å­¦ä¹ ä¸€ä¸ªæå°çš„å‘é‡ $v$ã€‚é€šè¿‡åœ¨ä¸åŒå±‚ä¹‹é—´å…±äº«è¿™ä¸ªå‘é‡ï¼ˆ**Tiling**ï¼‰ï¼Œå¯å°†å…¨ç½‘å¯è®­ç»ƒå‚æ•°å‹ç¼©è‡³ä¸ªä½æ•° [cite: 7, 175, 181]ã€‚
- **å…¬å¼**ï¼š
  $$W' = W + U \Sigma (\sum_{i=1}^{u} v_i P_i) V^\top$$
  [cite_start]å…¶ä¸­ $U, \Sigma, V$ æ¥è‡ªåŸæƒé‡çš„ SVD åˆ†è§£ï¼ˆå†»ç»“ï¼‰ï¼Œ$P$ æ˜¯å›ºå®šéšæœºæŠ•å½±ï¼Œ$v$ æ˜¯å”¯ä¸€çš„å¯è®­ç»ƒå‚æ•° [cite: 173, 174]ã€‚

### 2. ä¸ºä»€ä¹ˆå¿…é¡»æ˜¯ RLï¼Ÿ
[cite_start]è®ºæ–‡çš„æ ¸å¿ƒå‘ç°æ˜¯ï¼š**åœ¨å¦‚æ­¤æç«¯çš„å‚æ•°é™åˆ¶ä¸‹ï¼ˆ<100 å‚æ•°ï¼‰ï¼ŒSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å‡ ä¹å®Œå…¨å¤±æ•ˆï¼Œåªæœ‰ RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰èƒ½å¥æ•ˆ** [cite: 10, 65]ã€‚
- [cite_start]**SFT çš„å±€é™**ï¼šSFT å¼ºè¿«æ¨¡å‹è®°å¿†å‚è€ƒç­”æ¡ˆçš„æ ¼å¼å’Œé£æ ¼ï¼ˆ"Noise"ï¼‰ï¼Œè¿™éœ€è¦è¾ƒå¤§çš„å®¹é‡ [cite: 147, 148]ã€‚
- [cite_start]**RL çš„ä¼˜åŠ¿**ï¼šRL ä»…å…³æ³¨æœ€ç»ˆç»“æœçš„å¯¹é”™ï¼ˆ"Signal"ï¼‰ï¼Œå…è®¸æ¨¡å‹å¿½ç•¥æ— å…³ç»†èŠ‚ã€‚TinyLoRA æ­£æ˜¯åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨ä»…æœ‰ 13 ä¸ªå‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ GRPO ç®—æ³•åœ¨ GSM8K ä¸Šè¾¾åˆ°äº† 91% çš„å‡†ç¡®ç‡ [cite: 64, 149]ã€‚

### 3. æœ¬é¡¹ç›®çš„â€œé­”æ”¹â€é€‚é…
æˆ‘ä»¬éµå¾ªè®ºæ–‡çš„ç²¾ç¥å†…æ ¸ï¼Œä½†é’ˆå¯¹**ä»£ç ç”Ÿæˆä»»åŠ¡**å’Œ**æ¶ˆè´¹çº§æ˜¾å¡**è¿›è¡Œäº†é€‚é…ï¼š

| ç‰¹æ€§ | åŸè®ºæ–‡è®¾ç½® (Paper) | æœ¬é¡¹ç›®é€‚é… (Ours) |
| :--- | :--- | :--- |
| **ä»»åŠ¡é¢†åŸŸ** | [cite_start]æ•°å­¦æ¨ç† (GSM8K, MATH) [cite: 8] | **ä»£ç ç«èµ› (Luogu OJ)** |
| **åŸºåº§æ¨¡å‹** | [cite_start]Qwen2.5-7B / Llama-3 [cite: 64] | **Qwen2.5-Coder-3B-Instruct** |
| **å‚æ•°é‡** | 13 å‚æ•° ($u=13$) | **16 å‚æ•° ($u=16$)** |
| **ç²¾åº¦å¤„ç†** | [cite_start]BF16 / FP32 [cite: 8] | **4-bit é‡åŒ– (NF4) + åŠ¨æ€åé‡åŒ– SVD** |
| **å¥–åŠ±æœºåˆ¶** | ç­”æ¡ˆåŒ¹é… (Exact Match) | **g++ ç¼–è¯‘ + æµ‹è¯•ç”¨ä¾‹è¿è¡Œ (RLVR)** |
| **æ˜¾å­˜ä¼˜åŒ–** | éœ€é«˜æ˜¾å­˜ (A100/H100) | **é€‚é…å•å¡æ¶ˆè´¹çº§ GPU (16GB+)** |

> **å…³é”®å·¥ç¨‹æŒ‘æˆ˜**ï¼šåŸè®ºæ–‡æœªæ¶‰åŠ 4-bit é‡åŒ–æ¨¡å‹ã€‚æœ¬é¡¹ç›®é¢å¤–å®ç°äº†åœ¨åˆå§‹åŒ–é˜¶æ®µå¯¹ 4-bit æƒé‡è¿›è¡Œ `dequantize` è§£åŒ…ï¼Œåœ¨ CPU ä¸Šå®Œæˆ FP32 ç²¾åº¦çš„ SVD åˆ†è§£ï¼Œå†è½¬å› BF16 æ³¨å†Œä¸º Buffer çš„æµç¨‹ï¼Œä»è€Œåœ¨ä½æ˜¾å­˜ç¯å¢ƒä¸‹å®ç°äº† TinyLoRA åˆå§‹åŒ–ã€‚

## æ ¸å¿ƒç‰¹ç‚¹

- **æè‡´å‚æ•°å‹ç¼©**ï¼š
  - æ•´ä¸ªæ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°åªæœ‰ä¸€ä¸ªå‘é‡ `global_v âˆˆ R^{16}`ï¼›
  - å…¨ç½‘æ‰€æœ‰è¢«æ›¿æ¢çš„ Linear å±‚éƒ½å…±äº«è¿™ 16 ä¸ªæ ‡é‡ï¼›
  - ä½ å¯ä»¥é€šè¿‡è¿è¡Œ `train_rl.py` æˆ– `verify_pipeline.py` æ¥æŸ¥çœ‹æ¨¡å‹å‚æ•°ä¿¡æ¯ï¼ˆæ€»å‚æ•°é‡ / å¯è®­ç»ƒå‚æ•°é‡ / å‹ç¼©ç‡ï¼‰ã€‚

- **TinyLoRA Tiling**ï¼š
  - å¯¹åŸå§‹ Linear æƒé‡ï¼ˆåŒ…æ‹¬ 4bit é‡åŒ–æƒé‡ï¼‰åš SVD åˆ†è§£ï¼Œå¾—åˆ°å›ºå®šçš„éª¨æ¶ `U, S, Vh`ï¼›
  - å†é€šè¿‡éšæœºçŸ©é˜µç°‡ `P âˆˆ R^{uÃ—rÃ—r}` ä¸å…±äº«å‘é‡ `v âˆˆ R^u` é‡æ„ä¸€ä¸ªä½ç§©å¢é‡ï¼›
  - åªè®­ç»ƒ `v`ï¼Œå®ç°è®ºæ–‡ä¸­çš„ Tiling / å…¨å‚æ•°å…±äº«ã€‚

- **çœŸå®ä»£ç ç¯å¢ƒå¥–åŠ±**ï¼š
  - æŠŠæ¨¡å‹ç”Ÿæˆçš„ C++ ä»£ç å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼›
  - ä½¿ç”¨ç³»ç»Ÿ `g++` ç¼–è¯‘ï¼›
  - ä¸‰æ¡£ç¦»æ•£ rewardï¼šç¼–è¯‘å¤±è´¥=0ï¼Œç¼–è¯‘æˆåŠŸä½†æ ·ä¾‹é”™è¯¯=0.5ï¼Œé€šè¿‡æ ·ä¾‹=1.0ï¼›
  - ä»£ç ä¸é€šè¿‡ç¼–è¯‘ / è¶…æ—¶ / è¿è¡Œé”™è¯¯ -> reward ç›´æ¥è¶‹è¿‘äº 0ã€‚

- **æ˜¾å­˜å‹å¥½**ï¼š
  - åŸºåº§ä¸º 3B Coder æ¨¡å‹ï¼Œç»“åˆ bitsandbytes 4bit é‡åŒ– + BF16 è®¡ç®—ï¼›
  - åœ¨å•å¡æœ‰é™æ˜¾å­˜ç¯å¢ƒä¸‹ä¹Ÿèƒ½è·‘å®Œæ•´çš„ RL loopï¼ˆå½“ç„¶ï¼Œä¼šæ¯”è¾ƒæ…¢ï¼‰ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

å»ºè®®ä½¿ç”¨ Linux + Python 3.10 åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå¹¶ç¡®ä¿å·²å®‰è£… `g++` ç¼–è¯‘å™¨ã€‚

```bash
python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

> æç¤ºï¼š`requirements.txt` ä¸­å·²åŒ…å« `torch`ã€`transformers`ã€`datasets`ã€`trl`ã€`peft`ã€`bitsandbytes`ã€`modelscope` ç­‰ä¾èµ–ã€‚

### 2. ä¸‹è½½åŸºåº§æ¨¡å‹

`train_rl.py` ä¼šåœ¨æœ¬åœ°ä¸å­˜åœ¨æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨é€šè¿‡ ModelScope ä¸‹è½½ï¼š

- æ¨¡å‹ IDï¼š`qwen/Qwen2.5-Coder-3B-Instruct`
- é»˜è®¤æœ¬åœ°è·¯å¾„ï¼š`./models/Qwen2.5-Coder-3B-Instruct`

ä½ ä¹Ÿå¯ä»¥æ˜¾å¼è°ƒç”¨ï¼š

```python
from modelscope.hub.snapshot_download import snapshot_download

snapshot_download(
    repo_id="qwen/Qwen2.5-Coder-3B-Instruct",
    local_dir="./models/Qwen2.5-Coder-3B-Instruct",
)
```

### 3. å‡†å¤‡æ´›è°·æ•°æ®ï¼ˆDPO â†’ RLï¼‰

å‡è®¾ä½ å·²ç»æœ‰ä¸€ä¸ªä» Hugging Face / ModelScope ä¸‹è½½çš„æ´›è°· DPO æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ `datasets` çš„ `load_from_disk` ä¿å­˜åˆ°äº†æœ¬åœ° `./local_luogu_dpo/` ç›®å½•ï¼ˆç›®å½•ä¸‹å« `state.json` ç­‰æ–‡ä»¶ï¼‰ã€‚

è¿è¡Œï¼š

```bash
python convert_dataset.py
```

`convert_dataset.py` ä¼šï¼š

- ä» `./local_luogu_dpo` ä¸­è¯»å– `train` splitï¼›
- æå– `item["conversations"][0]["value"]` ä½œä¸ºé¢˜ç›®æè¿°ï¼›
- ç”¨æ­£åˆ™åœ¨é¢˜é¢ä¸­åŒ¹é…ã€Œæ ·ä¾‹è¾“å…¥ / è¾“å‡ºã€ä»£ç å—ï¼›
- ä¸¢å¼ƒåŒ…å«ä¸­æ–‡å­—ç¬¦çš„æ ·ä¾‹ï¼ˆåªä¿ç•™çº¯æ•°å­— / è‹±æ–‡ / ç¬¦å·çš„æ ·ä¾‹ï¼‰ï¼›
- å°†ç»“æœå†™å…¥ `./local_luogu_rl/luogu_rl_data.jsonl`ï¼›
- åŒæ—¶æŠŠæå–å¤±è´¥çš„é¢˜é¢å†™å…¥ `./local_luogu_rl/failed_extraction.jsonl` ä»¥ä¾¿äººå·¥æ’æŸ¥ã€‚

### 4. å¯é€‰ï¼šéªŒè¯æµæ°´çº¿ä¸æ•°æ®ä¸‹è½½

åœ¨çœŸæ­£è®­ç»ƒå‰ï¼Œå¯æ‰§è¡Œä»¥ä¸‹è„šæœ¬è¿›è¡Œæ£€æŸ¥ä¸å‡†å¤‡ï¼š

- ä¸‹è½½ DPO æ•°æ®é›†ï¼ˆå¦‚æœä½ è¿˜æ²¡ä¸‹è½½ï¼‰ï¼š

```bash
python download_dataset.py
```

- éªŒè¯ç«¯åˆ°ç«¯æµæ°´çº¿ï¼ˆæ¨¡å‹åŠ è½½ã€ç”Ÿæˆã€ä»£ç æå–ä¸ç¼–è¯‘è¿è¡Œçš„ç¤ºä¾‹ï¼‰ï¼š

```bash
python verify_pipeline.py
```

`verify_pipeline.py` ä¼šåŠ è½½ tokenizer å’Œæ¨¡å‹ï¼ˆè‹¥æœ¬åœ°ä¸å­˜åœ¨åˆ™ä½¿ç”¨è¿œç«¯ IDï¼‰ï¼Œå¯¹é¢„è®¾ JSON ç¤ºä¾‹ç”Ÿæˆä»£ç ã€æå–å¹¶å°è¯•ç¼–è¯‘è¿è¡Œæ ·ä¾‹ï¼Œä»è€Œå¸®åŠ©ä½ éªŒè¯ç¯å¢ƒæ˜¯å¦å®Œæ•´ï¼ˆä¾‹å¦‚æ˜¯å¦å®‰è£… `g++`ã€æ¨¡å‹ä¸ tokenizer é…ç½®æ˜¯å¦æ­£ç¡®ç­‰ï¼‰ã€‚

è¯´æ˜ TinyLoRA æ³¨å…¥ä¸å‚æ•°å†»ç»“é€»è¾‘æ˜¯æ­£å¸¸çš„ã€‚

### 5. å¯åŠ¨ RL è®­ç»ƒ

åŸºç¡€ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤u=16ï¼Œè®­ç»ƒå…¨éƒ¨æ•°æ®ï¼‰ï¼š

```bash
python train_rl.py
```

è‡ªå®šä¹‰ TinyLoRA å‚æ•°æ•°é‡ï¼ˆu å€¼ï¼‰ï¼š

```bash
python train_rl.py 32     # ä½¿ç”¨ u=32ï¼ˆ32 ä¸ªå¯è®­ç»ƒå‚æ•°ï¼‰
python train_rl.py 8      # ä½¿ç”¨ u=8ï¼ˆ8 ä¸ªå¯è®­ç»ƒå‚æ•°ï¼‰
```

é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ï¼š

```bash
python train_rl.py 16 100      # u=16ï¼Œä»…è®­ç»ƒå‰ 100 ä¸ªæ ·æœ¬
python train_rl.py 32 50       # u=32ï¼Œä»…è®­ç»ƒå‰ 50 ä¸ªæ ·æœ¬
python train_rl.py 16          # u=16ï¼Œè®­ç»ƒå…¨éƒ¨æ ·æœ¬ï¼ˆç¬¬äºŒä¸ªå‚æ•°å¯çœç•¥ï¼‰
```

> **å‚æ•°è¯´æ˜**ï¼š
> - **ç¬¬ä¸€ä¸ªå‚æ•°** `u`ï¼šTinyLoRA ä¸­å…±äº«å‘é‡ `global_v` çš„ç»´åº¦ï¼Œå³å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°ã€‚è‹¥ä¸æä¾›ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ `u=16`ã€‚
> - **ç¬¬äºŒä¸ªå‚æ•°** `MAX_SAMPLES`ï¼šæœ€å¤šè®­ç»ƒçš„æ ·æœ¬æ•°é‡ã€‚è‹¥ä¸æä¾›ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨æ•°æ®é›†ã€‚è¿™ä¸ªå‚æ•°åœ¨å¿«é€Ÿå®éªŒã€è°ƒè¯•è¶…å‚æ•°æˆ–æ˜¾å­˜ä¸è¶³æ—¶éå¸¸æœ‰ç”¨ã€‚ ç›®å‰æ˜¯shuffleåå–æ•°æ®é›†ï¼ˆåŸæ•°æ®é›†æ¯é“é¢˜ç›®ä¼šå‡ºç°2æ¬¡ï¼‰ï¼Œæ•…å¦‚æ­¤ã€‚ï¼ˆç›®å‰ç§å­æ˜¯å…¨å±€çš„TINYLORA_SEEDã€‚ï¼‰

`train_rl.py` å°†ä¼šï¼š

1. ç¡®ä¿åŸºåº§æ¨¡å‹å·²å‡†å¤‡å¥½ï¼ˆå¿…è¦æ—¶è‡ªåŠ¨ä¸‹è½½ï¼‰ï¼›
2. ä»¥ 4bit é‡åŒ–æ–¹å¼åŠ è½½ `Qwen2.5-Coder-3B-Instruct`ï¼›
3. æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»º u ç»´çš„å…±äº«å‘é‡ï¼›
4. æ³¨å…¥ TinyLoRA Tilingï¼ˆå…¨å±€å…±äº« `global_v`ï¼‰ï¼›
5. ä» `./local_luogu_rl/luogu_rl_data.jsonl` è¯»å– RL æ•°æ®ï¼›
6. è‹¥æŒ‡å®šäº† `MAX_SAMPLES`ï¼Œåˆ™ä»…é€‰å–å‰ N ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼›
7. ä½¿ç”¨ `GRPOTrainer` è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼›
8. è®­ç»ƒå®Œæˆåï¼Œå°†è®­ç»ƒç»“æœä¿å­˜ä¸º `output/tiny_lora_v.pt`ã€‚

**ä¿å­˜å†…å®¹**ï¼š`tiny_lora_v.pt` æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å«è¿˜åŸæ¨¡å‹æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ï¼š

```python
{
    "global_v": tensor([...]),     # è®­ç»ƒå¥½çš„ v å‘é‡ï¼Œshape=(u,)
    "u_value": 32,                 # v çš„ç»´åº¦
    "rank": 2,                     # TinyLoRA çš„ rank
    "seed": 42,                    # P çŸ©é˜µçš„éšæœºç§å­ï¼ˆç”¨äºå¤ç°ï¼‰
    "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",  # åŸºåº§æ¨¡å‹ ID
    "total_replaced_layers": 252,  # æ›¿æ¢çš„å±‚æ•°
}
```

> **è¿˜åŸæ–¹å¼**ï¼šåŠ è½½åŸºåº§æ¨¡å‹ â†’ ç”¨ç›¸åŒ `seed` å›ºå®šéšæœºç§å­ â†’ ç”¨ç›¸åŒ `u_value` å’Œ `rank` æ‰§è¡Œ `apply_tiny_lora` â†’ å°† `global_v` åŠ è½½å› `global_params.global_v`ã€‚ç§å­ç›¸åŒä¿è¯ P çŸ©é˜µå®Œå…¨ä¸€è‡´ï¼ŒSVD æ˜¯ç¡®å®šæ€§è¿ç®—æ‰€ä»¥ U/S/Vh ä¹Ÿä¸€è‡´ã€‚

å¦‚æœä½ æƒ³è‡ªå®šä¹‰è¾“å‡ºç›®å½•ï¼Œå¯ä»¥ä¿®æ”¹ `train_rl.py` é¡¶éƒ¨çš„ï¼š

```python
OUTPUT_DIR = "./output/luoguqwencoder-lora"
```

---

## æ•°æ®å‡†å¤‡ä¸æ ¼å¼

### ä¸Šæ¸¸æ•°æ®ï¼šæ´›è°·é¢˜ç›®ï¼ˆDPO ç‰ˆï¼‰

- **æ•°æ®æ¥æº**ï¼šHugging Face æ•°æ®é›† `Misaka114514/luogu_dpo`ï¼ˆhttps://huggingface.co/datasets/Misaka114514/luogu_dpoï¼‰ã€‚*åœ¨æ­¤è°¢è¿‡ï¼*

åŸå§‹æ•°æ®å½¢æ€å¤§è‡´ä¸ºï¼š

- å­—æ®µ `conversations`ï¼šä¸€ä¸ªå¯¹è¯åˆ—è¡¨ï¼›
- é€šå¸¸ `conversations[0]["value"]` æ˜¯é¢˜ç›®æè¿°ï¼ˆMarkdown é£æ ¼ï¼‰ï¼›
- é¢˜ç›®ä¸­åŒ…å«ç±»ä¼¼ï¼š

```markdown
**è¾“å…¥ï¼š**
```text
... æ ·ä¾‹è¾“å…¥ ...
```

**è¾“å‡ºï¼š**
```text
... æ ·ä¾‹è¾“å‡º ...
```
```

`convert_dataset.py` ä¼šä»è¿™é‡ŒæŠ½å–æµ‹è¯•ç”¨ä¾‹ã€‚

### RL è®­ç»ƒæ•°æ®ï¼šJSONL æ ¼å¼

`convert_dataset.py` ç”Ÿæˆçš„ `luogu_rl_data.jsonl` ä¸­ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€æ¡ JSONï¼Œå¯¹åº”ä¸€é¢˜ï¼š

```json
{
  "prompt": "<å®Œæ•´é¢˜ç›®æè¿°ï¼Œé€šå¸¸æ˜¯ Markdown æ–‡æœ¬>",
  "test_cases": [
    {"input": "<æ ·ä¾‹è¾“å…¥ 1>", "output": "<æ ·ä¾‹è¾“å‡º 1>"},
    {"input": "<æ ·ä¾‹è¾“å…¥ 2>", "output": "<æ ·ä¾‹è¾“å‡º 2>"}
  ]
}
```

åœ¨ `train_rl.py` ä¸­é€šè¿‡ï¼š

```python
from datasets import load_dataset

rl_dataset = load_dataset(
    "json",
    data_files="./local_luogu_rl/luogu_rl_data.jsonl",
    split="train",
)
```

ç›´æ¥ä½œä¸º `GRPOTrainer` çš„ `train_dataset` ä½¿ç”¨ã€‚

---

## è®­ç»ƒæµç¨‹ï¼ˆRL / GRPOï¼‰

æ ¸å¿ƒè®­ç»ƒé€»è¾‘ä½äº `train_rl.py`ï¼š

1. **æ¨¡å‹åŠ è½½ä¸é‡åŒ–**
   - ä½¿ç”¨ `BitsAndBytesConfig`ï¼š
     - `load_in_4bit=True`
     - `bnb_4bit_quant_type="nf4"`
     - `bnb_4bit_use_double_quant=True`
     - `bnb_4bit_compute_dtype=torch.float16`
   - é€šè¿‡ `device_map="auto"` å°†æ¨¡å‹è‡ªåŠ¨åˆ‡åˆ†åˆ°å¯ç”¨ GPUã€‚

2. **TinyLoRA æ³¨å…¥ä¸å‚æ•°å†»ç»“**
   - åˆ›å»ºå…¨å±€å…±äº«å‘é‡ï¼ˆç»´åº¦ç”±å‘½ä»¤è¡Œå‚æ•° `u` å†³å®šï¼Œé»˜è®¤16ï¼‰ï¼š
     - `global_v = nn.Parameter(torch.zeros(U_VALUE))`
   - é€šè¿‡ `apply_tiny_lora(model, global_v)`ï¼š
     - éå†æ¨¡å‹å­æ¨¡å—ï¼›
     - æ‰¾åˆ°åå­—ä»¥ `q_proj / k_proj / v_proj / o_proj / gate_proj / up_proj / down_proj` ç»“å°¾çš„ `nn.Linear`ï¼›
     - æ›¿æ¢ä¸º `TinyLoRALinear`ï¼›
   - éšåï¼š
     - ä»…ä¿ç•™ `global_v` çš„ `requires_grad=True`ï¼›
     - å…¶ä»–æ‰€æœ‰å‚æ•°å…¨éƒ¨ `requires_grad=False`ã€‚

3. **GRPO é…ç½®**

`train_rl.py` ä¸­ä½¿ç”¨çš„ç¤ºä¾‹è¶…å‚æ•°ï¼š

- `num_train_epochs=1`
- `per_device_train_batch_size=1`
- `gradient_accumulation_steps=8`
- `learning_rate=1e-5`
- `num_generations=4`ï¼ˆGroup Size Gï¼Œæ¯ä¸ªæ ·æœ¬é‡‡æ · 4 ä¸ªç­”æ¡ˆï¼‰
- `max_completion_length=512`
- `bf16=True`

ä½ å¯ä»¥æ ¹æ®æ˜¾å­˜ä¸è®­ç»ƒæ—¶é—´éœ€æ±‚è°ƒæ•´ä¸Šé¢çš„å‚æ•°ã€‚

4. **è®­ç»ƒå¾ªç¯**

GRPO çš„æ•´ä½“æµç¨‹ç®€è¦ä¸ºï¼š

- å¯¹äºæ¯ä¸ªæ ·æœ¬ `prompt`ï¼š
  1. é‡‡æ ·å¤šä¸ª `completions`ï¼ˆC++ ä»£ç ï¼‰ï¼›
  2. è°ƒç”¨ `code_reward_func` å¯¹æ¯ä¸ª completion ç¼–è¯‘ + è¿è¡Œï¼Œå¾—åˆ° rewardï¼›
  3. ä½¿ç”¨ GRPO ç®—æ³•æ ¹æ® reward æ›´æ–°ç­–ç•¥ï¼ˆè¿™é‡Œå°±æ˜¯æ›´æ–° 16 ç»´çš„ `global_v`ï¼‰ã€‚

---

## TinyLoRA Tiling æŠ€æœ¯ç»†èŠ‚

è‡ªå®šä¹‰å±‚ `TinyLoRALinear` çš„æ ¸å¿ƒæ€æƒ³ï¼š

1. å¯¹åŸå§‹æƒé‡çŸ©é˜µ `W âˆˆ R^{outÃ—in}` åš SVDï¼š

   $$W = U S V^H$$

   - å®ç°ä¸­å…ˆå°† 4bit æƒé‡åé‡åŒ–ä¸º `W_real`ï¼Œå†åœ¨ CPU ä¸Šåš `torch.linalg.svd`ï¼›
   - åªå–å‰ `rank=2` ä¸ªå¥‡å¼‚å€¼åŠå¯¹åº”çš„åˆ— / è¡Œï¼Œå¾—åˆ°ç²¾ç®€ç‰ˆ `U, S, V^H`ï¼›
   - è¿™äº›å¼ é‡é€šè¿‡ `register_buffer` æ³¨å†Œä¸º Bufferï¼Œä¸å‚ä¸è®­ç»ƒã€‚

2. å®šä¹‰å…¨å±€å…±äº«å‚æ•°ï¼š

   - `v âˆˆ R^u`ï¼Œå…¶ä¸­ `u=16`ï¼›
   - éšæœºåˆå§‹åŒ–ä¸€ç»„å›ºå®šçŸ©é˜µç°‡ `P âˆˆ R^{uÃ—rÃ—r}`ï¼›
   - æ„é€ ï¼š

     $$R = \sum_{i=1}^{u} v_i P_i \in R^{rÃ—r}$$

3. æ„é€ å¢é‡æƒé‡ï¼š

   - $$\Delta W = U S R V^H$$
   - å®é™…å‰å‘ä¸­è®¡ç®—ï¼š

     $$y = x W^T + x (\Delta W)^T$$

4. Tilingï¼ˆè·¨å±‚å…±äº«ï¼‰

   - æ¨¡å‹ä¸­æ‰€æœ‰ç›®æ ‡ `nn.Linear` å±‚éƒ½å…±äº«åŒä¸€ä¸ª `v`ï¼›
   - æ•´ä¸ªæ¨¡å‹åªæœ‰è¿™ä¸€ç»„ 16 ç»´å‚æ•°åœ¨æ›´æ–°ã€‚

ä½ å¯ä»¥é€šè¿‡ `verify_pipeline.py` æˆ–ç›´æ¥è§‚å¯Ÿ `train_rl.py` çš„å¯åŠ¨æ—¥å¿—æ¥ç¡®è®¤ TinyLoRA æ³¨å…¥æ˜¯å¦æ­£ç¡®å¹¶æ£€æŸ¥å¯è®­ç»ƒå‚æ•°é‡ã€‚

---

## å¥–åŠ±å‡½æ•°ï¼šç¼–è¯‘è¿è¡Œ C++ ä»£ç 

å¥–åŠ±å‡½æ•°å®ç°ä½äº `train_rl.py` ä¸­çš„ `code_reward_func` ä¸ `compile_and_run`ï¼š

1. **ä»æ¨¡å‹è¾“å‡ºä¸­æå–ä»£ç **
   - ä¼˜å…ˆåŒ¹é…å½¢å¦‚ï¼š

     ```markdown
          ```cpp
          // C++ ä»£ç 
          ```
     ```

   - è‹¥æ²¡æœ‰æ˜¾å¼ä»£ç å—ï¼Œåˆ™å›é€€ä¸ºåªè¦åŒ…å« `#include` çš„è£¸ä»£ç æ®µï¼›
   - è‹¥ä»æ— æ³•è¯†åˆ«ï¼Œåˆ™ç›´æ¥ç»™ 0 åˆ†ã€‚

2. **ç¼–è¯‘é˜¶æ®µ**
   - å°†ä»£ç å†™å…¥ä¸´æ—¶ç›®å½•ä¸­çš„ `solution.cpp`ï¼›
   - é€šè¿‡æ­£åˆ™åˆ é™¤ä»£ç ä¸­çš„ `freopen(...)` ç­‰æ–‡ä»¶é‡å®šå‘è¯­å¥ï¼Œæ”¹ç”¨æ ‡å‡†è¾“å…¥è¾“å‡ºï¼›
   - ä½¿ç”¨ï¼š

     ```bash
     g++ solution.cpp -o solution -O2
     ```

   - ç¼–è¯‘å¤±è´¥ / è¶…æ—¶ -> æœ¬æ¬¡æ ·æœ¬ reward = 0ã€‚

3. **è¿è¡Œé˜¶æ®µ**
   - å¯¹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼š
     - å°† `case["input"]` ä½œä¸º stdinï¼›
     - æ•è· stdoutï¼Œä¸ `case["output"]` è¿›è¡Œå­—ç¬¦ä¸²çº§æ¯”å¯¹ï¼ˆ`strip()` åï¼‰ï¼›
   - è¿è¡Œæœ‰è¶…æ—¶ä¿æŠ¤ï¼ˆä¾‹å¦‚ 2 ç§’ï¼‰ï¼Œé˜²æ­¢æ­»å¾ªç¯å¡æ­»è®­ç»ƒã€‚

4. **æ‰“åˆ†è§„åˆ™**

   å¥–åŠ±å‡½æ•°é‡‡ç”¨ä¸‰æ¡£è¯„åˆ†åˆ¶ï¼š

   - **ç¼–è¯‘å¤±è´¥** æˆ– **ä»£ç æ ¼å¼æ— æ•ˆ**ï¼š`reward = 0`
     - åŒ…æ‹¬ç¼–è¯‘é”™è¯¯ã€ç¼–è¯‘è¶…æ—¶ã€æ— æ³•æå–ä»£ç å—ç­‰æƒ…å†µï¼›
   
   - **ç¼–è¯‘æˆåŠŸä½†æµ‹è¯•ç”¨ä¾‹å¤±è´¥**ï¼š`reward = 0.5`
     - ä»£ç èƒ½é€šè¿‡ g++ ç¼–è¯‘ï¼Œä½†è¿è¡Œåä¸èƒ½é€šè¿‡å…¨éƒ¨æ ·ä¾‹æµ‹è¯•ï¼ˆå¯èƒ½é€šè¿‡éƒ¨åˆ†æˆ–å…¨éƒ¨å¤±è´¥ï¼‰ï¼›
   
   - **ç¼–è¯‘æˆåŠŸä¸”é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹**ï¼š`reward = 1.0`
     - ä»£ç æ—¢èƒ½ç¼–è¯‘æˆåŠŸï¼Œä¹Ÿèƒ½åœ¨æ‰€æœ‰æä¾›çš„æ ·ä¾‹ä¸Šäº§ç”Ÿæ­£ç¡®è¾“å‡ºã€‚

   **æ ¸å¿ƒå¼ºåŒ–ä¿¡å·**ï¼š
   - è¿™ç§è®¾è®¡é¼“åŠ±æ¨¡å‹å…ˆå­¦ä¼šç”Ÿæˆã€Œèƒ½ç¼–è¯‘çš„ä»£ç ã€ï¼ˆ0 â†’ 0.5 çš„è¿›æ­¥ï¼‰ï¼Œ
   - ç„¶ååœ¨ç¼–è¯‘åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–é€»è¾‘ä»¥é€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼ˆ0.5 â†’ 1.0 çš„è¿›æ­¥ï¼‰ã€‚
   - ç›¸æ¯”è¿ç»­æ‰“åˆ†ï¼Œç¦»æ•£ reward æä¾›äº†æ›´æ¸…æ™°çš„å­¦ä¹ é˜¶æ®µåˆ’åˆ†ã€‚

> è¿™æ„å‘³ç€æ¨¡å‹ä¸ä»…è¦ã€Œçœ‹èµ·æ¥åƒ C++ã€ï¼Œè¿˜è¦çœŸçš„èƒ½é€šè¿‡æ ·ä¾‹è¾“å…¥è¾“å‡ºï¼Œ
> å¼ºåŒ–ä¿¡å·æ¥è‡ªçœŸå®çš„ç¼–è¯‘å™¨ä¸è¿è¡Œç¯å¢ƒï¼Œè€Œéé™æ€æ‰“åˆ†ã€‚

---

## èµ„æºæ¶ˆè€—ä¸æ³¨æ„äº‹é¡¹

- **æ˜¾å­˜**ï¼š
  - 3B æ¨¡å‹ + 4bit é‡åŒ– + BF16 è®¡ç®—ï¼Œå•å¡ 16GB æ˜¾å­˜å¯ä»¥å°è¯•ï¼ˆä½†ä½™é‡ä¸ç®—å¤§ï¼‰ï¼›
  - RL + ç¼–è¯‘è¿è¡Œä¼šæ˜¾è‘—å¢åŠ æ—¶é—´æ¶ˆè€—ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šæ¯”ä¼ ç»Ÿ LoRA SFT æ…¢å¾ˆå¤šã€‚

- **æ“ä½œç³»ç»Ÿ**ï¼š
  - æ¨è Linux ç¯å¢ƒï¼ˆå½“å‰è„šæœ¬åœ¨ Linux ä¸‹å¼€å‘ä¸æµ‹è¯•ï¼‰ï¼›
  - éœ€è¦å¯ç”¨çš„ `g++`ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä¸´æ—¶ç›®å½•ä¸‹åˆ›å»ºä¸æ‰§è¡Œå¯æ‰§è¡Œæ–‡ä»¶ã€‚

- **å®‰å…¨**ï¼š
  - å¼ºçƒˆä¸å»ºè®®å¯¹ä¸å—ä¿¡ä»»çš„æ•°æ®é›†è¿è¡Œæ­¤å¥–åŠ±å‡½æ•°ï¼›
  - æœ¬é¡¹ç›®çš„å‡è®¾æ˜¯ã€Œæ•°æ®é›†æ¥æºå¯ä¿¡ã€ä¸”ä»…ç”¨äºç ”ç©¶ç¯å¢ƒã€‚

---

## å¼€æºä¸è®¸å¯è¯

- æœ¬ä»“åº“è„šæœ¬é»˜è®¤é‡‡ç”¨ MIT è®¸å¯è¯ï¼ˆè§ `LICENSE`ï¼‰ã€‚
- åŸºåº§æ¨¡å‹ `Qwen2.5-Coder-3B-Instruct` ç”±ç¬¬ä¸‰æ–¹ï¼ˆQwen å›¢é˜Ÿï¼‰æä¾›ï¼Œè¯·éµå®ˆå…¶åŸå§‹è®¸å¯è¯ï¼›
- æœ¬ä»“åº“ä¸åˆ†å‘å®Œæ•´åŸºåº§æ¨¡å‹æƒé‡ï¼Œåªæä¾›ï¼š
  - TinyLoRA / RL ç›¸å…³ä»£ç ï¼›
  - æ•°æ®å¤„ç†è„šæœ¬ï¼›
  - å¯é€‰çš„ TinyLoRA å‚æ•°æ–‡ä»¶ï¼ˆ`tiny_lora_v.pt`ï¼Œå†…å«è®­ç»ƒå¥½çš„ `global_v` å‘é‡åŠé‡å»ºæ¨¡å‹æ‰€éœ€çš„å…ƒä¿¡æ¯ï¼šu å€¼ã€rankã€éšæœºç§å­ç­‰ï¼‰ã€‚

---

## å¼•ç”¨



```bibtex
@article{morris2026learning,
  title={Learning to Reason in 13 Parameters},
  author={Morris, John X and Mireshghallah, Niloofar and Ibrahim, Mark and Mahloujifar, Saeed},
  journal={arXiv preprint arXiv:2602.04118},
  year={2026}
}
```

---

## English Version

### LuoguQwen-RL â€” TinyLoRA Experiment

LuoguQwen-RL is an evolution of the original [LuoguQwen SFT project](https://github.com/Chi-Shan0707/Qwen4Luogu-SFT).

The goal of LuoguQwen-RL is:
> Under the constraints of limited VRAM (3B model + 4bit quantization) and extreme parameter compression (only 16 parameters), train Qwen2.5-Coder through Reinforcement Learning (RL) to generate C++ code that passes sample tests on Luogu competitive programming problems.

This repository is an **unofficial reproduction and adaptation of the TinyLoRA paper**:
- `theory/README.md` provides theoretical insights into TinyLoRA / GRPO.
- We extend TinyLoRA from mathematical reasoning (GSM8K) to **code generation + compile-and-run rewards**.
- While the paper uses 7B models with 13 parameters, we use a 3B Coder model with 16 parameters, maintaining the "extreme low-rank + global sharing" core philosophy.

**Core Scripts:**
- `train_rl.py`: Main training script using 4-bit `Qwen2.5-Coder-3B-Instruct`, TinyLoRA Tiling, and `GRPOTrainer` with `g++` rewards.
- `convert_dataset.py`: Extracts `prompt` and `test_cases` from local Luogu DPO data (Markdown style) and filters non-ASCII samples.
- `download_dataset.py`: Downloads Luogu DPO dataset from Hugging Face.
- `verify_pipeline.py`: Validates the end-to-end flow of model loading, generation, extraction, and compilation.

### Paper Reproduction

This project is based on the paper **"Learning to Reason in 13 Parameters" (Morris et al., 2026)**.

#### 1. Core Theory: TinyLoRA
TinyLoRA is an extreme parameter-efficient fine-tuning method that breaks the rank limits of traditional LoRA ($O(d \times r)$). By freezing the original weight's characteristic directions ($U, V$) via SVD and learning only a tiny shared vector $v$ (**Tiling**), it compresses trainable parameters to single digits.

#### 2. Why Reinforcement Learning?
At such extreme parameter scales (<100 parameters), Supervised Fine-Tuning (SFT) often fails because it forces the model to memorize noise (formatting/styles). RL instead focuses on the "Signal" (correctness), allowing the model to ignore irrelevant details and succeed even with minimal capacity.

#### 3. Adaptation Table

| Feature | Paper Setting | Our Adaptation |
| :--- | :--- | :--- |
| **Domain** | Math Reasoning (GSM8K, MATH) | **Code Competitions (Luogu OJ)** |
| **Base Model** | Qwen2.5-7B / Llama-3 | **Qwen2.5-Coder-3B-Instruct** |
| **Parameters** | 13 parameters ($u=13$) | **16 parameters ($u=16$)** |
| **Precision** | BF16 / FP32 | **4-bit (NF4) + Dynamic Dequant SVD** |
| **Reward** | Exact Match | **g++ Compile + Test Case Execution** |
| **Optimization**| High-end GPUs (A100/H100) | **Consumer GPUs (16GB+ VRAM)** |

### Core Features

- **Extreme Parameter Compression**: The entire model's trainable parameters consist of a single vector `global_v âˆˆ R^{16}` shared across all replaced linear layers.
- **TinyLoRA Tiling**: Freezes the base skeleton (`U, S, Vh`) from SVD and reconstructs low-rank increments via a shared vector `v`.
- **Real-world Code Reward**:
  - Compiles generated code with system `g++`.
  - Discrete rewards: `0` for failure, `0.5` for compilation success, `1.0` for passing all test cases.
- **VRAM Friendly**: Optimized for 16GB+ single-GPU setups using 4-bit quantization and BF16 computation.

### Quick Start

#### 1. Environment
Requires Linux, Python 3.10+, and `g++`.
```bash
pip install -r requirements.txt
```

#### 2. Model Download
`train_rl.py` auto-downloads `qwen/Qwen2.5-Coder-3B-Instruct` to `./models/` if missing.

#### 3. Data Preparation
```bash
python download_dataset.py  # Download DPO data
python convert_dataset.py   # Convert to RL (JSONL) format
```

#### 4. Verification
```bash
python verify_pipeline.py    # Verify the model->generate->compile loop
```

#### 5. Start RL Training
```bash
python train_rl.py [u] [MAX_SAMPLES]
```
- **`u`**: Shared vector dimension (default 16).
- **`MAX_SAMPLES`**: Max number of samples to train (default all).

Training saves `output/tiny_lora_v.pt` containing `global_v` and reconstruction metadata (seed, rank, model_id).

### Data Preparation and Format

- **Source**: DPO-formatted Luogu data â€” Hugging Face dataset `Misaka114514/luogu_dpo` (https://huggingface.co/datasets/Misaka114514/luogu_dpo).*Thank you, Misaka114514!*
- **RL Format**: Each line in `luogu_rl_data.jsonl` contains:
  - `prompt`: Problem statement in Markdown.
  - `test_cases`: List of dictionary pairs with `input` and `output`.

### Training Process (RL / GRPO)

1. **Quantization**: Loads base model in 4-bit (NF4) with double quantization.
2. **Injection**: Replaces `q_proj`, `k_proj`, etc., with `TinyLoRALinear`.
3. **GRPO**: Uses Group Relative Policy Optimization. Each prompt samples multiple completions (G=4), evaluated by the `code_reward_func`.

### TinyLoRA Tiling Technical Details

- **SVD Integration**: 4-bit weights are dequantized to FP32 on CPU for SVD decomposition. The top components are stored as frozen buffers.
- **Increment Construction**: $\Delta W = U S (\sum_{i=1}^{u} v_i P_i) V^H$, where $P$ is a fixed random projection cluster.
- **Global Sharing**: Every injected layer references the same `global_v`.

### Reward Function: Compile and Run C++ Code

1. **Extraction**: Regex matching for code blocks or standard `#include` snippets.
2. **Compilation**: Strips `freopen` to use standard I/O; runs `g++ -O2`.
3. **Scoring Logic**:
   - `0`: Compilation error or invalid format.
   - `0.5`: Successfully compiled but failed tests (partial or full).
   - `1.0`: Successfully passed all sample cases.
   This provides a clear gradient: Learn to compile first, then learn to solve.

### Resource Consumption and Notes

- **VRAM**: 16GB is the baseline recommendation.
- **Performance**: Slower than SFT due to overhead from sampling and external compiler calls.
- **Safety**: Reward function executes compiled binaries; only use with trusted datasets.

### License and Citation

- Code: MIT License.
- Base Model: Qwen License.

If you find this project useful, please consider citing the original paper:

```bibtex
@article{morris2026learning,
  title={Learning to Reason in 13 Parameters},
  author={Morris, John X and Mireshghallah, Niloofar and Ibrahim, Mark and Mahloujifar, Saeed},
  journal={arXiv preprint arXiv:2602.04118},
  year={2026}
}
```