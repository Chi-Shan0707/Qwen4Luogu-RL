import torch
import torch.nn as nn
import os
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import prepare_model_for_kbit_training
from trl import GRPOTrainer, GRPOConfig
from modelscope.hub.snapshot_download import snapshot_download
import bitsandbytes as bnb


# ========== æ¨¡å‹é…ç½® ==========
MS_MODEL_ID = "qwen/Qwen2.5-Coder-3B-Instruct"
LOCAL_MODEL_DIR = "./models/Qwen2.5-Coder-3B-Instruct"
OUTPUT_DIR = "./output/luoguqwencoder-lora"

#  Qwen2.5-Coder-3B-Instruct
# ========== ä¸‹è½½æ¨¡å‹ ==========
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"ä»ModelScopeä¸‹è½½æ¨¡å‹ {MS_MODEL_ID} åˆ° {LOCAL_MODEL_DIR}...")
    snapshot_download(
        repo_id=MS_MODEL_ID,
        local_dir=LOCAL_MODEL_DIR,
    )
    print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
else:
    print(f"æœ¬åœ°å·²å­˜åœ¨æ¨¡å‹ï¼Œç›´æ¥åŠ è½½ï¼š{LOCAL_MODEL_DIR}")

# ========== åŠ è½½ tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    trust_remote_code=True,
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# ========== åŠ è½½æ¨¡å‹ï¼ˆ4bit é‡åŒ–ï¼‰==========
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    # torch_dtype=torch.bfloat16,
    dtype=torch.bfloat16,
)
model.config.use_cache = False

# å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ
model = prepare_model_for_kbit_training(model)



# ========== å®šä¹‰ TinyLoRA å±‚ ==========

# è·å–æ¨¡å‹ç¬¬ä¸€å±‚çš„è®¾å¤‡ (é€šå¸¸æ˜¯ cuda:0)
device = model.model.layers[0].self_attn.q_proj.weight.device
print(f"æ¨¡å‹ä¸»è®¾å¤‡: {device}")

# ã€ä¿®å¤é”™è¯¯1ã€‘åˆ›å»ºä¸€ä¸ª wrapper module æ¥æ­£ç¡®æ³¨å†Œ global_v
class TinyLoRAGlobalParams(nn.Module):
    """ä¸“é—¨ç”¨äºæ³¨å†Œå…¨å±€å…±äº«å‘é‡çš„å®¹å™¨"""
    def __init__(self, u_dim=16, device='cpu', dtype=torch.bfloat16):
        super().__init__()
        # è¿™æ ·æ³¨å†Œæ‰ä¼šè¢« model.named_parameters() è¯†åˆ«
        self.global_v = nn.Parameter(torch.zeros(u_dim, device=device, dtype=dtype))
    
    def forward(self):
        # å®¹å™¨æ¨¡å—ä¸éœ€è¦å®é™…çš„å‰å‘é€»è¾‘
        return self.global_v

# åˆ›å»ºå…¨å±€å‚æ•°å®¹å™¨
global_params = TinyLoRAGlobalParams(u_dim=16, device=device, dtype=torch.bfloat16)


class TinyLoRALinear(nn.Module):
    def __init__(self, original_layer, rank = 2, u = 16, global_params_ref=None):
    # R= v_1 P_1 + v_2 P_2 + ... + v_u P_u
    # véƒ½æ˜¯scalar
    # Péƒ½æ˜¯rank x rankçš„çŸ©é˜µ
    # global_params_ref: æŒ‡å‘åŒ…å« global_v çš„å®¹å™¨æ¨¡å—

        super().__init__()
        # å¿…å…ˆç»§æ‰¿çˆ¶ç±»çš„åˆå§‹åŒ–å‡½æ•°ï¼Œæ‰èƒ½ä½¿ç”¨ nn.Module çš„åŠŸèƒ½ï¼ˆä¾‹å¦‚æ³¨å†Œå‚æ•°å’Œç¼“å†²åŒºï¼‰ã€‚
        
        #  super().__init__() æ˜¯ä»€ä¹ˆï¼Ÿ
        # è¿™æ˜¯ Python é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼ˆOOPï¼‰çš„æ ‡å‡†å†™æ³•ã€‚
        # å«ä¹‰ï¼šè°ƒç”¨çˆ¶ç±»ï¼ˆParent Classï¼‰çš„åˆå§‹åŒ–å‡½æ•°ã€‚
        # åœ¨è¿™é‡Œçš„ä½œç”¨ï¼šä½ çš„ç±» TinyLoRALinear ç»§æ‰¿è‡ª nn.Moduleï¼ˆPyTorch çš„ç¥ç»ç½‘ç»œåŸºç±»ï¼‰ã€‚æ‰§è¡Œ super().__init__() æ˜¯ä¸ºäº†è®© PyTorch çš„æœºåˆ¶ç”Ÿæ•ˆï¼Œæ¯”å¦‚ï¼š
        # æ³¨å†Œä½ å®šä¹‰çš„ self.v ä¸ºå¯è®­ç»ƒå‚æ•°ã€‚
        # æ³¨å†Œ self.U, self.S ç­‰ä¸º Bufferï¼ˆä¸è®­ç»ƒçš„å‚æ•°ï¼‰ã€‚


        print(f"original_layer.device: {original_layer.weight.device}, dtype: {original_layer.weight.dtype}")

        original_device = original_layer.weight.device # è®°å½•åŸdevice


        self.base_layer = original_layer
        
        # ã€å…³é”®ä¿®å¤ã€‘ä¿å­˜å¯¹ global_params å®¹å™¨çš„å¼•ç”¨ï¼Œè€Œä¸æ˜¯ç›´æ¥æŒæœ‰å‚æ•°
        # è¿™æ · v åªè¢« model.tiny_lora_params æŒæœ‰ä¸€æ¬¡ï¼Œé¿å…é‡å¤æ³¨å†Œé—®é¢˜
        if global_params_ref is None:
            raise RuntimeError("å¿…é¡»ä¼ å…¥ global_params_refï¼")
        self.global_params_ref = global_params_ref

        W = original_layer.weight.data.float()
        if hasattr(original_layer.weight, "quant_state"):
            # ã€ä¿®å¤é”™è¯¯2ã€‘4-bit æƒ…å†µï¼šæ˜¾å¼ä¼ å…¥ quant_type
            W_real = bnb.functional.dequantize_4bit(
                original_layer.weight.data, 
                original_layer.weight.quant_state,
                quant_type="nf4"  # ä¸ BitsAndBytesConfig ä¸­çš„é…ç½®ä¸€è‡´
            )
        else:
            # éé‡åŒ–æƒ…å†µ
            W_real = original_layer.weight.data


        W_real_on_cpu = W_real.float().cpu()

        U, S ,Vh = torch.linalg.svd( W_real_on_cpu ,full_matrices=False)

        # SVD åˆ†è§£ W çŸ©é˜µ
        # W = U S Vh 
        # Vhæ˜¯ Vçš„Hermitian transposedï¼Œå…±è½­è½¬ç½®
        # å†»ç»“ U, S, V (LoRA-XS çš„éª¨æ¶)

        

        # å°†ç»“æœè½¬å› BFloat16 å¹¶ç§»å› GPU
        # æˆªæ–­å¹¶æ³¨å†Œ(å³å›ºå®šä½)
        # å»ºè®®è½¬å› bf16 çœæ˜¾å­˜
        # 
        # è¿™ä¸€æ­¥ä¹Ÿæ˜¯ä¸ºäº†è®© TinyLoRA çš„å‚æ•°å’Œä¸»æ¨¡å‹ç²¾åº¦ä¿æŒä¸€è‡´
        
        target_dtype = torch.bfloat16

        self.register_buffer('U', U[:, :rank].to(original_device).to(target_dtype)) 
        self.register_buffer('S', torch.diag(S[:rank]).to(original_device).to(target_dtype))
        self.register_buffer('Vh', Vh[:rank, :].to(original_device).to(target_dtype))
        
        # å›ºå®šéšæœºçŸ©é˜µ P  (For TinyLoRA)
        self.register_buffer('P', torch.randn(u, rank, rank, device=original_device, dtype=target_dtype))

    def forward(self, x):
        # ã€å…³é”®ä¿®å¤ã€‘åŠ¨æ€ä»å®¹å™¨ä¸­è·å– global_vï¼Œè€Œä¸æ˜¯ä½œä¸ºè‡ªå·±çš„å±æ€§
        # è¿™æ ·ç¡®ä¿ v åªè¢« model.tiny_lora_params æ³¨å†Œä¸€æ¬¡
        v = self.global_params_ref.global_v
        
        # è®¡ç®— TinyLoRA çš„å¢é‡çŸ©é˜µ R = sum_i(v_i * P_i)
        # æ³¨æ„ï¼šä¸èƒ½ç”¨ 'u, urr -> rr'ï¼Œå› ä¸º einsum è¾“å‡ºä¸­åŒä¸€ä¸‹æ ‡ä¸èƒ½é‡å¤
        # å¿…é¡»ç”¨ä¸åŒå­—æ¯åŒºåˆ†ä¸¤ä¸ª rank ç»´åº¦
        R = torch.einsum('u, uij -> ij', v, self.P)
        # é‡ç»„å¢é‡æƒé‡
        delta_W = self.U @ self.S @ R @ self.Vh
        # å‰å‘ä¼ æ’­ï¼šx * (W + delta_W)^T
        return self.base_layer(x) + x @ delta_W.t()


def apply_tiny_lora(model, global_params_ref):
    """
    éå†æ¨¡å‹ï¼Œå°†æ‰€æœ‰ç›®æ ‡ Linear å±‚æ›¿æ¢ä¸º TinyLoRALinearï¼Œ
    å¹¶ä¼ å…¥å¯¹ global_params å®¹å™¨çš„å¼•ç”¨ï¼Œå®ç°è®ºæ–‡ä¸­çš„ Tiling (å…¨å‚æ•°å…±äº«)ã€‚
    """
    # Qwen/Llama çš„ç›®æ ‡æ¨¡å—åç§°é€šå¸¸åŒ…å«è¿™äº›
    target_suffixes = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    # è®¡æ•°å™¨
    replaced_count = 0
    
    # é€’å½’å‡½æ•°ï¼šéå†å­æ¨¡å—
    for name, child in model.named_children():
        # å¦‚æœæ˜¯ç›®æ ‡ Linear å±‚
        if isinstance(child, (nn.Linear, bnb.nn.Linear4bit)) and any(name.endswith(s) for s in target_suffixes):
            # 1. åˆ›å»º TinyLoRA å±‚ï¼Œä¼ å…¥ global_params å®¹å™¨çš„å¼•ç”¨
            new_layer = TinyLoRALinear(child, rank=2, u=16, global_params_ref=global_params_ref)
            
            # 2. æ›¿æ¢æ‰åŸæ¨¡å— (Monkey Patch)
            setattr(model, name, new_layer)
            replaced_count += 1
            print(f"å·²æ›¿æ¢: {name} -> TinyLoRA (Shared)")
            
        else:
            # ç»§ç»­é€’å½’éå†å­æ¨¡å— (ä¾‹å¦‚ model.layers.0.self_attn...)
            replaced_count += apply_tiny_lora(child, global_params_ref)
            
    return replaced_count

# ========== æ‰§è¡Œæ›¿æ¢ ==========
print("æ­£åœ¨åº”ç”¨ TinyLoRA Tiling (å‚æ•°å…±äº«)...")

# ã€å…³é”®ä¿®å¤ã€‘å…ˆå°† global_params æ³¨å†Œä¸ºæ¨¡å‹çš„å­æ¨¡å—
# è¿™æ ·åœ¨å±‚æ›¿æ¢æ—¶ï¼ŒTinyLoRALinear å°±èƒ½é€šè¿‡å¼•ç”¨è®¿é—®åˆ°å·²æ³¨å†Œçš„ global_v
model.tiny_lora_params = global_params
print(f"âœ… å·²å°† global_params æ³¨å†Œåˆ°æ¨¡å‹")

# ç„¶åå†è¿›è¡Œå±‚æ›¿æ¢ï¼Œä¼ å…¥ global_params å®¹å™¨æœ¬èº«
total_replaced = apply_tiny_lora(model, global_params)
print(f"æ›¿æ¢å®Œæˆï¼å…±æ›¿æ¢äº† {total_replaced} ä¸ªæ¨¡å—ã€‚")

# ========== å…³é”®æ­¥éª¤ï¼šå†»ç»“é™¤ v ä»¥å¤–çš„æ‰€æœ‰å‚æ•° ==========
print("æ­£åœ¨å†»ç»“æ¨¡å‹å‚æ•°...")

# ã€æ›´ä¼˜é›…çš„æ–¹æ¡ˆã€‘ç›´æ¥é€šè¿‡å¯¹è±¡å¼•ç”¨æ“ä½œï¼Œä¸ä¾èµ–å­—ç¬¦ä¸²åŒ¹é…
# 1. ç¬¬ä¸€æ­¥ï¼šå…¨å±€å†»ç»“æ‰€æœ‰å‚æ•°
model.requires_grad_(False)

# 2. ç¬¬äºŒæ­¥ï¼šç²¾å‡†è§£å†» global_v
# ç›´æ¥é€šè¿‡å¯¹è±¡å¼•ç”¨æ“ä½œï¼Œç»å¯¹ç¨³å¥
global_params.global_v.requires_grad = True
print(f"âœ… å¯è®­ç»ƒå‚æ•°: global_v, shape={global_params.global_v.shape}")

# éªŒè¯å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"\næ€»å‚æ•°é‡: {all_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params}")
if trainable_params != 16:
    raise RuntimeError(f"è­¦å‘Šï¼šå¯è®­ç»ƒå‚æ•°æ•°é‡ä¸º {trainable_params}ï¼Œé¢„æœŸä¸º 16ï¼")

import re
import subprocess
import tempfile
import os

def compile_and_run(code, test_cases):
    """
    ç¼–è¯‘å¹¶è¿è¡Œä»£ç ï¼Œè¿”å›é€šè¿‡ç‡ (0.0 ~ 1.0)
    """
    code = re.sub(r'freopen\s*\(.*?\);', '', code, flags=re.IGNORECASE)
    # 1. åˆ›å»ºä¸´æ—¶ç›®å½• (ç”¨å®Œå³åˆ ï¼Œé˜²æ­¢åƒåœ¾æ–‡ä»¶å †ç§¯)
    with tempfile.TemporaryDirectory() as temp_dir:
        src_file = os.path.join(temp_dir, "solution.cpp")
        exe_file = os.path.join(temp_dir, "solution")
        
        # 2. å†™å…¥ C++ ä»£ç 
        with open(src_file, 'w', encoding='utf-8') as f:
            f.write(code)
            
        # 3. ç¼–è¯‘ (åŠ ä¸Š -O2 ä¼˜åŒ–ï¼Œä¸”ä¸é“¾æ¥å¤šä½™åº“)
        # timeout=5 é˜²æ­¢ç¼–è¯‘å™¨å¡æ­»
        try:
            compile_result = subprocess.run(
                ['g++', src_file, '-o', exe_file, '-O2'],
                capture_output=True, text=True, timeout=5
            )
            if compile_result.returncode != 0:
                return 0.0 # ç¼–è¯‘å¤±è´¥
        except subprocess.TimeoutExpired:
            return 0.0 # ç¼–è¯‘è¶…æ—¶

        # 4. è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        passed_count = 0
        total_cases = len(test_cases)
        
        if total_cases == 0:
            return 0.0

        for case in test_cases:
            input_data = case['input']
            expected_output = case['output'].strip()
            
            try:
                # å…³é”®ï¼šä½¿ç”¨ input=input_data æ¨¡æ‹Ÿ freopen/cin
                # timeout=2 ç§’ï¼Œé˜²æ­¢æ­»å¾ªç¯ (éå¸¸é‡è¦ï¼ï¼ï¼)
                run_result = subprocess.run(
                    [exe_file],
                    input=input_data,
                    capture_output=True,
                    text=True,
                    timeout=2 
                )
                
                # è·å–æ¨¡å‹è¾“å‡ºå¹¶å»é¦–å°¾ç©ºæ ¼
                actual_output = run_result.stdout.strip()
                
                # ç®€å•æ¯”å¯¹ (ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦æ”¹æˆæµ®ç‚¹æ•°æ¯”å¯¹ç­‰)
                if actual_output == expected_output:
                    passed_count += 1
                    
            except subprocess.TimeoutExpired:
                pass # è¿è¡Œè¶…æ—¶ç®—é”™
            except Exception:
                pass # è¿è¡Œæ—¶é”™è¯¯(RE)ç®—é”™

        return passed_count / total_cases

def code_reward_func(completions, test_cases, **kwargs):
    """
    GRPO è¦æ±‚çš„ Reward Function æ ¼å¼
    completions: list[str], æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªå›å¤
    test_cases: list[list[dict]], å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆæ³¨æ„ GRPO ä¼ è¿›æ¥çš„æ˜¯ batchï¼‰
    """
    rewards = []
    
    # éå†æ¯ä¸€æ¡ç”Ÿæˆçš„å›å¤
    for completion, cases in zip(completions, test_cases):
        # 1. æå–ä»£ç å—
        # åŒ¹é… ```cpp ... ``` æˆ– ``` ... ```
        match = re.search(r"```(?:cpp|c\+\+)?\n(.*?)```", completion, re.DOTALL)
        
        if not match:
            # å¦‚æœæ²¡æå–åˆ°ï¼Œå°è¯•æ‰¾ä¸€ä¸‹æ˜¯å¦æœ‰è£¸ä»£ç ï¼ˆåŒ…å« #includeï¼‰
            if "#include" in completion:
                code = completion
            else:
                rewards.append(0.0) # æ ¼å¼å®Œå…¨ä¸å¯¹
                continue
        else:
            code = match.group(1)

        # 2. è¯„æµ‹
        score = compile_and_run(code, cases)
        rewards.append(score)
        
    return rewards
    

# ã€ä¿®å¤é”™è¯¯4 - æœ€ç»ˆæ–¹æ¡ˆã€‘ç»•è¿‡ Trainer å¯¹çº¯é‡åŒ–æ¨¡å‹çš„æ£€æŸ¥
# Trainer çš„æ£€æŸ¥é€»è¾‘ (transformers/trainer.py):
#   _is_quantized_and_base_model = model.is_quantized AND NOT model._hf_peft_config_loaded
#   if _is_quantized_and_base_model and not isinstance(model, PeftModel): raise ValueError
#
# æˆ‘ä»¬çš„ TinyLoRA æ˜¯åˆæ³•çš„ adapterï¼ˆåªè®­ç»ƒ 16 ä¸ªå‚æ•°ï¼‰ï¼Œä½†ä¸æ˜¯æ ‡å‡† PeftModelã€‚
# è®¾ç½® _hf_peft_config_loaded = True è®©ç¬¬ä¸€é“æ£€æŸ¥ç›´æ¥ä¸º Falseï¼Œä¸ä¼šèµ°åˆ° isinstance åˆ¤æ–­ã€‚
# è¿™ä¸å½±å“å®é™…è®¡ç®—â€”â€”æƒé‡å·²ç»åœ¨å†…å­˜ä¸­é‡åŒ–ï¼ŒTinyLoRA å±‚æ­£ç¡®å¤„ç†äº†åé‡åŒ–ã€‚
model._hf_peft_config_loaded = True

print("âœ… å·²è®¾ç½® _hf_peft_config_loaded=Trueï¼Œç»•è¿‡ Trainer çš„é‡åŒ–æ¨¡å‹æ£€æŸ¥")



# ========== åŠ è½½æ•°æ®é›† ==========


# å½“ä½ ä½¿ç”¨ load_dataset("json", data_files="....jsonl") æ—¶ï¼Œ
# Hugging Face ä¼šé»˜è®¤æŠŠä½ æä¾›çš„è¿™ä¸ªæ–‡ä»¶å½’ç±»ä¸º train åˆ†åŒºï¼ˆè¿™æ˜¯å®ƒçš„é»˜è®¤è¡Œä¸ºï¼‰ã€‚

# æ³¨æ„ï¼šdata_files æŒ‡å‘ä½  convert_dataset.py ç”Ÿæˆçš„å…·ä½“æ–‡ä»¶è·¯å¾„
# split="train" å¾ˆé‡è¦ï¼å› ä¸º load_dataset é»˜è®¤è¿”å› DatasetDictï¼Œ
# è€Œ Trainer éœ€è¦çš„æ˜¯ Dataset å¯¹è±¡ï¼ŒæŒ‡å®š split="train" ç›´æ¥æ‹¿åˆ°æ•°æ®ã€‚
rl_dataset = load_dataset(
    "json", 
    data_files="./local_luogu_rl/luogu_rl_data.jsonl", 
    # ç¡®è®¤è¿™é‡Œçš„è·¯å¾„å’Œä½  convert_dataset.py é‡Œçš„ OUTPUT_FILE ä¸€è‡´
    split="train"
)

# 2. (å¯é€‰) æ‰“å°ä¸€æ¡æ•°æ®éªŒè¯ä¸€ä¸‹
print(f"æ•°æ®åŠ è½½æˆåŠŸï¼æ ·æœ¬æ•°é‡: {len(rl_dataset)}")
print(f"æ ·ä¾‹æ•°æ®: {rl_dataset[0]}")
# ========== é…ç½®å¹¶å¯åŠ¨ GRPO è®­ç»ƒ ==========
# é…ç½® GRPO
training_args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=1, # å•å¡æ˜¾å­˜ä¸å¤Ÿå°±è®¾ä¸º 1
    gradient_accumulation_steps=8, # ç´¯ç§¯æ¢¯åº¦æ¥æ¨¡æ‹Ÿå¤§ Batch
    learning_rate=1e-5,            # RL å­¦ä¹ ç‡é€šå¸¸è¦å°
    num_generations=4,             # Group Size (G): æ¯æ¬¡é‡‡æ · 4 ä¸ªç­”æ¡ˆ
    max_completion_length=512,     # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
    logging_steps=1,
    bf16=True,                     # å¼€å¯ BF16 åŠ é€Ÿ
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = GRPOTrainer(
    model=model,
    reward_funcs=code_reward_func, # ä½ çš„åˆ¤é¢˜å‡½æ•°
    args=training_args,
    train_dataset=rl_dataset,   # å¤„ç†å¥½çš„æ•°æ®
    processing_class=tokenizer,    # Tokenizer
)

# å¼€å§‹è®­ç»ƒï¼
print("ğŸš€ å¼€å§‹ TinyLoRA-RL è®­ç»ƒ...")
trainer.train()

# ä¿å­˜ LoRA (åªä¿å­˜é‚£ä¸ª v å‘é‡)
# æ³¨æ„ï¼špeft çš„ save_pretrained å¯èƒ½ä¸è®¤ä½ çš„è‡ªå®šä¹‰å±‚
# æ‰‹åŠ¨ä¿å­˜ global_v
torch.save(global_params.global_v, f"{OUTPUT_DIR}/tiny_lora_v.pt")
print("è®­ç»ƒå®Œæˆï¼Œå‚æ•°å·²ä¿å­˜ï¼")