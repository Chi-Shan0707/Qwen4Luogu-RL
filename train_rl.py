import torch
import torch.nn as nn
import os
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model
from trl import GRPOTrainer, GRPOConfig
from modelscope.hub.snapshot_download import snapshot_download
import optimum
import bitsandbytes as bnb
from peft import prepare_model_for_kbit_training


# ========== æ¨¡å‹é…ç½® ==========
MS_MODEL_ID = "qwen/Qwen2.5-Coder-3B-Instruct"
LOCAL_MODEL_DIR = "./models/Qwen2.5-Coder-3B-Instruct"
OUTPUT_DIR = "./output/luoguqwencoder-lora"

#  Qwen2.5-Coder-3B-Instruct
# ========== ä¸‹è½½æ¨¡å‹ ==========
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"ä»ModelScopeä¸‹è½½æ¨¡å‹ {MS_MODEL_ID} åˆ° {LOCAL_MODEL_DIR}...")
    snapshot_download(
        repo_id=MS_MODEL_ID,
        local_dir=LOCAL_MODEL_DIR,
    )
    print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
else:
    print(f"æœ¬åœ°å·²å­˜åœ¨æ¨¡å‹ï¼Œç›´æ¥åŠ è½½ï¼š{LOCAL_MODEL_DIR}")

# ========== åŠ è½½ tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    trust_remote_code=True,
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# ========== åŠ è½½æ¨¡å‹ï¼ˆ4bit é‡åŒ–ï¼‰==========
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    # torch_dtype=torch.bfloat16,
    dtype=torch.bfloat16,
)
model.config.use_cache = False

# å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ
model = prepare_model_for_kbit_training(model)



# ========== å®šä¹‰ TinyLoRA å±‚ ==========

# è·å–æ¨¡å‹ç¬¬ä¸€å±‚çš„è®¾å¤‡ (é€šå¸¸æ˜¯ cuda:0)
device = model.model.layers[0].self_attn.q_proj.weight.device
print(f"æ¨¡å‹ä¸»è®¾å¤‡: {device}")

# ç›´æ¥åœ¨ GPU ä¸Šåˆ›å»º global_vï¼Œå¹¶è®¾ä¸º bfloat16
global_v = nn.Parameter(torch.zeros(16, device=device, dtype=torch.bfloat16))
global_v.requires_grad = True


class TinyLoRALinear(nn.Module):
    def __init__(self, original_layer, rank = 2, u = 16, shared_v =None):
    # R= v_1 P_1 + v_2 P_2 + ... + v_u P_u
    # véƒ½æ˜¯scalar
    # Péƒ½æ˜¯rank x rankçš„çŸ©é˜µ

        super().__init__()
        # å¿…å…ˆç»§æ‰¿çˆ¶ç±»çš„åˆå§‹åŒ–å‡½æ•°ï¼Œæ‰èƒ½ä½¿ç”¨ nn.Module çš„åŠŸèƒ½ï¼ˆä¾‹å¦‚æ³¨å†Œå‚æ•°å’Œç¼“å†²åŒºï¼‰ã€‚
        
        #  super().__init__() æ˜¯ä»€ä¹ˆï¼Ÿ
        # è¿™æ˜¯ Python é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼ˆOOPï¼‰çš„æ ‡å‡†å†™æ³•ã€‚
        # å«ä¹‰ï¼šè°ƒç”¨çˆ¶ç±»ï¼ˆParent Classï¼‰çš„åˆå§‹åŒ–å‡½æ•°ã€‚
        # åœ¨è¿™é‡Œçš„ä½œç”¨ï¼šä½ çš„ç±» TinyLoRALinear ç»§æ‰¿è‡ª nn.Moduleï¼ˆPyTorch çš„ç¥ç»ç½‘ç»œåŸºç±»ï¼‰ã€‚æ‰§è¡Œ super().__init__() æ˜¯ä¸ºäº†è®© PyTorch çš„æœºåˆ¶ç”Ÿæ•ˆï¼Œæ¯”å¦‚ï¼š
        # æ³¨å†Œä½ å®šä¹‰çš„ self.v ä¸ºå¯è®­ç»ƒå‚æ•°ã€‚
        # æ³¨å†Œ self.U, self.S ç­‰ä¸º Bufferï¼ˆä¸è®­ç»ƒçš„å‚æ•°ï¼‰ã€‚


        print(f"original_layer.device: {original_layer.weight.device}, dtype: {original_layer.weight.dtype}")

        original_device = original_layer.weight.device # è®°å½•åŸdevice


        self.base_layer = original_layer

        W = original_layer.weight.data.float()
        if hasattr(original_layer.weight, "quant_state"):
            # 4-bit æƒ…å†µ
            W_real = bnb.functional.dequantize_4bit(
                original_layer.weight.data, 
                original_layer.weight.quant_state
            )
        else:
            # éé‡åŒ–æƒ…å†µ
            W_real = original_layer.weight.data


        W_real_on_cpu = W_real.float().cpu()

        U, S ,Vh = torch.linalg.svd( W_real_on_cpu ,full_matrices=False)

        # SVD åˆ†è§£ W çŸ©é˜µ
        # W = U S Vh 
        # Vhæ˜¯ Vçš„Hermitian transposedï¼Œå…±è½­è½¬ç½®
        # å†»ç»“ U, S, V (LoRA-XS çš„éª¨æ¶)

        

        # å°†ç»“æœè½¬å› BFloat16 å¹¶ç§»å› GPU
        # æˆªæ–­å¹¶æ³¨å†Œ(å³å›ºå®šä½)
        # å»ºè®®è½¬å› bf16 çœæ˜¾å­˜
        # 
        # è¿™ä¸€æ­¥ä¹Ÿæ˜¯ä¸ºäº†è®© TinyLoRA çš„å‚æ•°å’Œä¸»æ¨¡å‹ç²¾åº¦ä¿æŒä¸€è‡´
        
        target_dtype = torch.bfloat16

        self.register_buffer('U', U[:, :rank].to(original_device).to(target_dtype)) 
        self.register_buffer('S', torch.diag(S[:rank]).to(original_device).to(target_dtype))
        self.register_buffer('Vh', Vh[:rank, :].to(original_device).to(target_dtype))
        
        # å›ºå®šéšæœºçŸ©é˜µ P  (For TinyLoRA)
        self.register_buffer('P', torch.randn(u, rank, rank, device=original_device, dtype=target_dtype))
        
        # å”¯ä¸€çš„å¯è®­ç»ƒå‚æ•° v (å¦‚æœä¼ å…¥ shared_v åˆ™å®ç°å‚æ•°å…±äº«)

        if shared_v is not None:
            # ä¸¥æŸ¥è®¾å¤‡æ˜¯å¦ä¸€è‡´
            if shared_v.device != original_device:
                raise RuntimeError(
                    f"è®¾å¤‡ä¸åŒ¹é…ï¼shared_v åœ¨ {shared_v.device}, "
                    f"ä½†å½“å‰å±‚åœ¨ {original_device}ã€‚\n"
                    "åœ¨å•å¡è®­ç»ƒä¸­ï¼Œè¯·ç¡®ä¿ global_v å’Œæ¨¡å‹éƒ½åœ¨åŒä¸€å¼ å¡ä¸Šã€‚"
                )
            
            # ç›´æ¥å¼•ç”¨ï¼ä¸è¦ cloneï¼Œä¸è¦ nn.Parameter
            self.v = shared_v 
        else:
            self.v = nn.Parameter(torch.zeros(u, device=original_device, dtype=target_dtype))

    def forward(self, x):
        # è®¡ç®— TinyLoRA çš„å¢é‡çŸ©é˜µ R
        R = torch.einsum('u, urr -> rr', self.v, self.P)
        # é‡ç»„å¢é‡æƒé‡
        delta_W = self.U @ self.S @ R @ self.Vh
        # å‰å‘ä¼ æ’­ï¼šx * (W + delta_W)^T
        return self.base_layer(x) + x @ delta_W.t()


def apply_tiny_lora(model, shared_v):
    """
    éå†æ¨¡å‹ï¼Œå°†æ‰€æœ‰ç›®æ ‡ Linear å±‚æ›¿æ¢ä¸º TinyLoRALinearï¼Œ
    å¹¶å¼ºåˆ¶ä½¿ç”¨åŒä¸€ä¸ª shared_vï¼Œå®ç°è®ºæ–‡ä¸­çš„ Tiling (å…¨å‚æ•°å…±äº«)ã€‚
    """
    # Qwen/Llama çš„ç›®æ ‡æ¨¡å—åç§°é€šå¸¸åŒ…å«è¿™äº›
    target_suffixes = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    # è®¡æ•°å™¨
    replaced_count = 0
    
    # é€’å½’å‡½æ•°ï¼šéå†å­æ¨¡å—
    for name, child in model.named_children():
        # å¦‚æœæ˜¯ç›®æ ‡ Linear å±‚
        if isinstance(child, nn.Linear) and any(name.endswith(s) for s in target_suffixes):
            # 1. åˆ›å»º TinyLoRA å±‚ï¼Œä¼ å…¥ global_v
            # æ³¨æ„ï¼šoriginal_layer=childï¼Œshared_v=shared_v
            new_layer = TinyLoRALinear(child, rank=2, u=16, shared_v=shared_v)
            
            # 2. æ›¿æ¢æ‰åŸæ¨¡å— (Monkey Patch)
            setattr(model, name, new_layer)
            replaced_count += 1
            print(f"å·²æ›¿æ¢: {name} -> TinyLoRA (Shared)")
            
        else:
            # ç»§ç»­é€’å½’éå†å­æ¨¡å— (ä¾‹å¦‚ model.layers.0.self_attn...)
            apply_tiny_lora(child, shared_v)
            
    return replaced_count

# ========== æ‰§è¡Œæ›¿æ¢ ==========
print("æ­£åœ¨åº”ç”¨ TinyLoRA Tiling (å‚æ•°å…±äº«)...")
# global_v å·²ç»åœ¨ä½ ä¹‹å‰çš„ä»£ç ä¸­å®šä¹‰äº†
total_replaced = apply_tiny_lora(model, global_v)
print(f"æ›¿æ¢å®Œæˆï¼å…±æ›¿æ¢äº† {total_replaced} ä¸ªæ¨¡å—ã€‚")

# ========== å…³é”®æ­¥éª¤ï¼šå†»ç»“é™¤ v ä»¥å¤–çš„æ‰€æœ‰å‚æ•° ==========
print("æ­£åœ¨å†»ç»“æ¨¡å‹å‚æ•°...")

for name, param in model.named_parameters():
    # åªæœ‰ global_v éœ€è¦æ¢¯åº¦ï¼Œå…¶ä»–å…¨éƒ¨å†»ç»“
    # æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬æ˜¯æŠŠ shared_v ä¼ è¿›å»çš„ï¼Œid(param) == id(global_v)
    if param is global_v:
        param.requires_grad = True
    else:
        param.requires_grad = False

import re
import subprocess
import tempfile

import subprocess
import tempfile
import re
import os

def compile_and_run(code, test_cases):
    """
    ç¼–è¯‘å¹¶è¿è¡Œä»£ç ï¼Œè¿”å›é€šè¿‡ç‡ (0.0 ~ 1.0)
    """
    code = re.sub(r'freopen\s*\(.*?\);', '', code, flags=re.IGNORECASE)
    # 1. åˆ›å»ºä¸´æ—¶ç›®å½• (ç”¨å®Œå³åˆ ï¼Œé˜²æ­¢åƒåœ¾æ–‡ä»¶å †ç§¯)
    with tempfile.TemporaryDirectory() as temp_dir:
        src_file = os.path.join(temp_dir, "solution.cpp")
        exe_file = os.path.join(temp_dir, "solution")
        
        # 2. å†™å…¥ C++ ä»£ç 
        with open(src_file, 'w', encoding='utf-8') as f:
            f.write(code)
            
        # 3. ç¼–è¯‘ (åŠ ä¸Š -O2 ä¼˜åŒ–ï¼Œä¸”ä¸é“¾æ¥å¤šä½™åº“)
        # timeout=5 é˜²æ­¢ç¼–è¯‘å™¨å¡æ­»
        try:
            compile_result = subprocess.run(
                ['g++', src_file, '-o', exe_file, '-O2'],
                capture_output=True, text=True, timeout=5
            )
            if compile_result.returncode != 0:
                return 0.0 # ç¼–è¯‘å¤±è´¥
        except subprocess.TimeoutExpired:
            return 0.0 # ç¼–è¯‘è¶…æ—¶

        # 4. è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        passed_count = 0
        total_cases = len(test_cases)
        
        if total_cases == 0:
            return 0.0

        for case in test_cases:
            input_data = case['input']
            expected_output = case['output'].strip()
            
            try:
                # å…³é”®ï¼šä½¿ç”¨ input=input_data æ¨¡æ‹Ÿ freopen/cin
                # timeout=2 ç§’ï¼Œé˜²æ­¢æ­»å¾ªç¯ (éå¸¸é‡è¦ï¼ï¼ï¼)
                run_result = subprocess.run(
                    [exe_file],
                    input=input_data,
                    capture_output=True,
                    text=True,
                    timeout=2 
                )
                
                # è·å–æ¨¡å‹è¾“å‡ºå¹¶å»é¦–å°¾ç©ºæ ¼
                actual_output = run_result.stdout.strip()
                
                # ç®€å•æ¯”å¯¹ (ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦æ”¹æˆæµ®ç‚¹æ•°æ¯”å¯¹ç­‰)
                if actual_output == expected_output:
                    passed_count += 1
                    
            except subprocess.TimeoutExpired:
                pass # è¿è¡Œè¶…æ—¶ç®—é”™
            except Exception:
                pass # è¿è¡Œæ—¶é”™è¯¯(RE)ç®—é”™

        return passed_count / total_cases

def code_reward_func(completions, test_cases, **kwargs):
    """
    GRPO è¦æ±‚çš„ Reward Function æ ¼å¼
    completions: list[str], æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªå›å¤
    test_cases: list[list[dict]], å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆæ³¨æ„ GRPO ä¼ è¿›æ¥çš„æ˜¯ batchï¼‰
    """
    rewards = []
    
    # éå†æ¯ä¸€æ¡ç”Ÿæˆçš„å›å¤
    for completion, cases in zip(completions, test_cases):
        # 1. æå–ä»£ç å—
        # åŒ¹é… ```cpp ... ``` æˆ– ``` ... ```
        match = re.search(r"```(?:cpp|c\+\+)?\n(.*?)```", completion, re.DOTALL)
        
        if not match:
            # å¦‚æœæ²¡æå–åˆ°ï¼Œå°è¯•æ‰¾ä¸€ä¸‹æ˜¯å¦æœ‰è£¸ä»£ç ï¼ˆåŒ…å« #includeï¼‰
            if "#include" in completion:
                code = completion
            else:
                rewards.append(0.0) # æ ¼å¼å®Œå…¨ä¸å¯¹
                continue
        else:
            code = match.group(1)

        # 2. è¯„æµ‹
        score = compile_and_run(code, cases)
        rewards.append(score)
        
    return rewards

# ========== åŠ è½½æ•°æ®é›† ==========


# å½“ä½ ä½¿ç”¨ load_dataset("json", data_files="....jsonl") æ—¶ï¼Œ
# Hugging Face ä¼šé»˜è®¤æŠŠä½ æä¾›çš„è¿™ä¸ªæ–‡ä»¶å½’ç±»ä¸º train åˆ†åŒºï¼ˆè¿™æ˜¯å®ƒçš„é»˜è®¤è¡Œä¸ºï¼‰ã€‚

# æ³¨æ„ï¼šdata_files æŒ‡å‘ä½  convert_dataset.py ç”Ÿæˆçš„å…·ä½“æ–‡ä»¶è·¯å¾„
# split="train" å¾ˆé‡è¦ï¼å› ä¸º load_dataset é»˜è®¤è¿”å› DatasetDictï¼Œ
# è€Œ Trainer éœ€è¦çš„æ˜¯ Dataset å¯¹è±¡ï¼ŒæŒ‡å®š split="train" ç›´æ¥æ‹¿åˆ°æ•°æ®ã€‚
rl_dataset = load_dataset(
    "json", 
    data_files="./local_luogu_rl/luogu_rl_data.jsonl", 
    # ç¡®è®¤è¿™é‡Œçš„è·¯å¾„å’Œä½  convert_dataset.py é‡Œçš„ OUTPUT_FILE ä¸€è‡´
    split="train"
)

# 2. (å¯é€‰) æ‰“å°ä¸€æ¡æ•°æ®éªŒè¯ä¸€ä¸‹
print(f"æ•°æ®åŠ è½½æˆåŠŸï¼æ ·æœ¬æ•°é‡: {len(rl_dataset)}")
print(f"æ ·ä¾‹æ•°æ®: {rl_dataset[0]}")
# ========== é…ç½®å¹¶å¯åŠ¨ GRPO è®­ç»ƒ ==========
# é…ç½® GRPO
training_args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=1, # å•å¡æ˜¾å­˜ä¸å¤Ÿå°±è®¾ä¸º 1
    gradient_accumulation_steps=8, # ç´¯ç§¯æ¢¯åº¦æ¥æ¨¡æ‹Ÿå¤§ Batch
    learning_rate=1e-5,            # RL å­¦ä¹ ç‡é€šå¸¸è¦å°
    num_generations=4,             # Group Size (G): æ¯æ¬¡é‡‡æ · 4 ä¸ªç­”æ¡ˆ
    max_completion_length=512,     # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
    logging_steps=1,
    bf16=True,                     # å¼€å¯ BF16 åŠ é€Ÿ
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = GRPOTrainer(
    model=model,
    reward_funcs=code_reward_func, # ä½ çš„åˆ¤é¢˜å‡½æ•°
    args=training_args,
    train_dataset=rl_dataset,   # å¤„ç†å¥½çš„æ•°æ®
    processing_class=tokenizer,    # Tokenizer
)

# å¼€å§‹è®­ç»ƒï¼
print("ğŸš€ å¼€å§‹ TinyLoRA-RL è®­ç»ƒ...")
trainer.train()

# ä¿å­˜ LoRA (åªä¿å­˜é‚£ä¸ª v å‘é‡)
# æ³¨æ„ï¼špeft çš„ save_pretrained å¯èƒ½ä¸è®¤ä½ çš„è‡ªå®šä¹‰å±‚
# ä½ å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¿å­˜ global_v
torch.save(global_v, f"{OUTPUT_DIR}/tiny_lora_v.pt")
print("è®­ç»ƒå®Œæˆï¼Œå‚æ•°å·²ä¿å­˜ï¼")